{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836d5ec1",
   "metadata": {},
   "source": [
    "# Projet INSEE - MOBILITÉ TRANSFRONTALIÈRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133f1843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] TEST DU FORMAT DU FICHIER\n",
      "Première ligne (extrait) :\n",
      "COMMUNE;ARM;DCFLT;DCLT;AGEREVQ;GS;DEROU;DIPL;EMPL;ILT;ILTUU;IMMI;INATC;INEEM;INPOM;INPSM;IPONDI;LPRM;METRODOM;MOCO;NA5;NPERR;REGION;REGLT;SEXE;STAT;STOCD;TP;TRANS;TYPL;TYPMR;VOIT;DEPT;DEPCOM;CANTVILLE;is_unique_key;NUMMI;ACHLR;AEMMR;AGED;AGER20;AGEREV;ANAI;ANEMR;APAF;ASCEN;BAIN;BATI;CATIRIS;CATL;CAT\n",
      "';' : 105  | ',' : 0  | '\\t' : 0\n",
      "\n",
      "[2] LECTURE ROBUSTE DU CSV\n",
      "✔ Dimensions : (494483, 106)\n",
      "✔ Colonnes (20 premières) : ['COMMUNE', 'ARM', 'DCFLT', 'DCLT', 'AGEREVQ', 'GS', 'DEROU', 'DIPL', 'EMPL', 'ILT', 'ILTUU', 'IMMI', 'INATC', 'INEEM', 'INPOM', 'INPSM', 'IPONDI', 'LPRM', 'METRODOM', 'MOCO']\n",
      "\n",
      "[3] VÉRIFICATIONS CRITIQUES\n",
      "ILT présent : True\n",
      "DCFLT présent : True\n",
      "Distribution ILT :\n",
      "ILT\n",
      "1    158593\n",
      "2    253635\n",
      "3     27098\n",
      "4     10840\n",
      "5        46\n",
      "6         7\n",
      "7     44264\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# ÉTAPE 1 — CHARGEMENT ROBUSTE DE LA BDD (ANTI-PIÈGES INSEE)\n",
    "# ============================================================\n",
    "\n",
    "INPUT_PATH = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/BDD_FINAL_CLEAN.csv\"\n",
    "\n",
    "print(\"\\n[1] TEST DU FORMAT DU FICHIER\")\n",
    "with open(INPUT_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    head = f.readline()\n",
    "\n",
    "print(\"Première ligne (extrait) :\")\n",
    "print(head[:300])\n",
    "print(\"';' :\", head.count(\";\"), \" | ',' :\", head.count(\",\"), \" | '\\\\t' :\", head.count(\"\\t\"))\n",
    "\n",
    "print(\"\\n[2] LECTURE ROBUSTE DU CSV\")\n",
    "df = pd.read_csv(\n",
    "    INPUT_PATH,\n",
    "    sep=None,            # auto-détection\n",
    "    engine=\"python\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "print(\"✔ Dimensions :\", df.shape)\n",
    "print(\"✔ Colonnes (20 premières) :\", list(df.columns[:20]))\n",
    "\n",
    "# ============================================================\n",
    "# VERROUILLAGE DES VARIABLES CLÉS\n",
    "# ============================================================\n",
    "\n",
    "# Sécurité noms de colonnes\n",
    "if \"DCFLT\" not in df.columns and \"DCLT\" in df.columns:\n",
    "    df = df.rename(columns={\"DCLT\": \"DCFLT\"})\n",
    "\n",
    "# Typage strict\n",
    "df[\"ILT\"] = pd.to_numeric(df[\"ILT\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df[\"DCFLT\"] = df[\"DCFLT\"].astype(\"string\")\n",
    "\n",
    "print(\"\\n[3] VÉRIFICATIONS CRITIQUES\")\n",
    "print(\"ILT présent :\", \"ILT\" in df.columns)\n",
    "print(\"DCFLT présent :\", \"DCFLT\" in df.columns)\n",
    "print(\"Distribution ILT :\")\n",
    "print(df[\"ILT\"].value_counts(dropna=False).sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bfffd2",
   "metadata": {},
   "source": [
    "# Création de variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a115f484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Chargement ==\n",
      "OK: 494,483 lignes, 106 colonnes\n",
      "== ID_INDIVIDU ==\n",
      "OK: ID uniques = True\n",
      "== TRANSFRONTALIER ==\n",
      "OK: transfrontaliers = 44264\n",
      "== WORK_IN_PAYS ==\n",
      "== Dummies pays ==\n",
      "== Checks cohérence ==\n",
      "ILT=7 mais DCFLT=99999 : 0\n",
      "Répartition WORK_IN_PAYS (transfrontaliers uniquement) :\n",
      "WORK_IN_PAYS\n",
      "Luxembourg                   23563\n",
      "Allemagne                    10135\n",
      "Suisse                        9407\n",
      "Belgique                       965\n",
      "Autre_etranger_ou_inconnu      188\n",
      "Monaco                           6\n",
      "Name: count, dtype: int64\n",
      "== Export CSV ==\n",
      "OK: /Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/BDD_FINAL_new.csv\n",
      "== Export Excel sample 10k ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ZipFile.__del__ at 0x110c8ea20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/zipfile/__init__.py\", line 1940, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.12/zipfile/__init__.py\", line 1957, in close\n",
      "ValueError: seek of closed file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: /Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/BDD_FINAL_new_sample10k.xlsx\n",
      "== Terminé ==\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# PARAMÈTRES (adapte tes chemins)\n",
    "# ============================================================\n",
    "INPUT_PATH = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/BDD_FINAL_CLEAN.csv\"\n",
    "OUTPUT_CSV_PATH = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/BDD_FINAL_new.csv\"\n",
    "OUTPUT_XLSX_SAMPLE_PATH = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/BDD_FINAL_new_sample10k.xlsx\"\n",
    "\n",
    "SAMPLE_N = 10_000\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ============================================================\n",
    "# 1) CHARGEMENT (ton fichier est en ; -> obligatoire)\n",
    "# ============================================================\n",
    "print(\"== Chargement ==\")\n",
    "df = pd.read_csv(INPUT_PATH, sep=\";\", encoding=\"utf-8\", low_memory=False)\n",
    "print(f\"OK: {df.shape[0]:,} lignes, {df.shape[1]} colonnes\")\n",
    "\n",
    "# Sécurités\n",
    "if \"DCFLT\" not in df.columns and \"DCLT\" in df.columns:\n",
    "    df = df.rename(columns={\"DCLT\": \"DCFLT\"})\n",
    "\n",
    "for col in [\"ILT\", \"DCFLT\"]:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Colonne manquante: {col}\")\n",
    "\n",
    "# Types propres\n",
    "df[\"ILT\"] = pd.to_numeric(df[\"ILT\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df[\"DCFLT\"] = df[\"DCFLT\"].astype(\"string\").str.strip().str.upper()\n",
    "\n",
    "# ============================================================\n",
    "# 2) ID\n",
    "# ============================================================\n",
    "print(\"== ID_INDIVIDU ==\")\n",
    "df[\"ID_INDIVIDU\"] = np.arange(1, len(df) + 1, dtype=np.int32)\n",
    "print(\"OK: ID uniques =\", df[\"ID_INDIVIDU\"].is_unique)\n",
    "\n",
    "# ============================================================\n",
    "# 3) TRANSFRONTALIER\n",
    "# ============================================================\n",
    "print(\"== TRANSFRONTALIER ==\")\n",
    "df[\"TRANSFRONTALIER\"] = (df[\"ILT\"] == 7).astype(\"int8\")\n",
    "print(\"OK: transfrontaliers =\", int(df[\"TRANSFRONTALIER\"].sum()))\n",
    "\n",
    "# ============================================================\n",
    "# 4) WORK_IN_PAYS (vectorisé, rapide)\n",
    "# ============================================================\n",
    "print(\"== WORK_IN_PAYS ==\")\n",
    "\n",
    "dc = df[\"DCFLT\"]\n",
    "\n",
    "# Par défaut\n",
    "work = np.full(len(df), \"Autre_etranger_ou_inconnu\", dtype=object)\n",
    "\n",
    "# France\n",
    "mask_fr = (dc == \"99999\")\n",
    "work[mask_fr] = \"France\"\n",
    "\n",
    "# Préfixes\n",
    "work[dc.str.startswith(\"LU\", na=False)] = \"Luxembourg\"\n",
    "work[dc.str.startswith(\"AL\", na=False)] = \"Allemagne\"\n",
    "work[dc.str.startswith(\"SU\", na=False)] = \"Suisse\"\n",
    "work[dc.str.startswith(\"BE\", na=False)] = \"Belgique\"\n",
    "work[dc.str.startswith(\"MO\", na=False)] = \"Monaco\"\n",
    "\n",
    "df[\"WORK_IN_PAYS\"] = work\n",
    "\n",
    "# IMPORTANT: pour éviter les faux positifs, on force les non-transfrontaliers à France\n",
    "df.loc[df[\"TRANSFRONTALIER\"] == 0, \"WORK_IN_PAYS\"] = \"France\"\n",
    "\n",
    "# ============================================================\n",
    "# 5) DUMMIES PAYS\n",
    "# ============================================================\n",
    "print(\"== Dummies pays ==\")\n",
    "df[\"WORK_LUXEMBOURG\"] = (df[\"WORK_IN_PAYS\"] == \"Luxembourg\").astype(\"int8\")\n",
    "df[\"WORK_ALLEMAGNE\"]  = (df[\"WORK_IN_PAYS\"] == \"Allemagne\").astype(\"int8\")\n",
    "df[\"WORK_SUISSE\"]     = (df[\"WORK_IN_PAYS\"] == \"Suisse\").astype(\"int8\")\n",
    "df[\"WORK_BELGIQUE\"]   = (df[\"WORK_IN_PAYS\"] == \"Belgique\").astype(\"int8\")\n",
    "\n",
    "# ============================================================\n",
    "# 6) CHECKS (peu de prints, pas de spam)\n",
    "# ============================================================\n",
    "print(\"== Checks cohérence ==\")\n",
    "incoh_ilt7_fr = df[(df[\"TRANSFRONTALIER\"] == 1) & (df[\"DCFLT\"] == \"99999\")]\n",
    "print(\"ILT=7 mais DCFLT=99999 :\", len(incoh_ilt7_fr))\n",
    "\n",
    "print(\"Répartition WORK_IN_PAYS (transfrontaliers uniquement) :\")\n",
    "print(df.loc[df[\"TRANSFRONTALIER\"] == 1, \"WORK_IN_PAYS\"].value_counts())\n",
    "\n",
    "# ============================================================\n",
    "# 7) EXPORT CSV enrichi (en ; pour rester cohérent Excel/INSEE)\n",
    "# ============================================================\n",
    "print(\"== Export CSV ==\")\n",
    "df.to_csv(OUTPUT_CSV_PATH, index=False, sep=\";\")\n",
    "print(\"OK:\", OUTPUT_CSV_PATH)\n",
    "\n",
    "# ============================================================\n",
    "# 8) EXPORT EXCEL SAMPLE 10 000 lignes\n",
    "# ============================================================\n",
    "print(\"== Export Excel sample 10k ==\")\n",
    "sample_df = df.sample(n=min(SAMPLE_N, len(df)), random_state=RANDOM_STATE).copy()\n",
    "\n",
    "# Colonnes à mettre devant pour vérif rapide\n",
    "front = [\"ID_INDIVIDU\", \"ILT\", \"DCFLT\", \"TRANSFRONTALIER\", \"WORK_IN_PAYS\",\n",
    "         \"WORK_LUXEMBOURG\", \"WORK_ALLEMAGNE\", \"WORK_SUISSE\", \"WORK_BELGIQUE\",\n",
    "         \"REGLT\", \"DEPT\", \"DEPCOM\", \"CANTVILLE\"]\n",
    "front = [c for c in front if c in sample_df.columns]\n",
    "sample_df = sample_df[front + [c for c in sample_df.columns if c not in front]]\n",
    "\n",
    "sample_df.to_excel(OUTPUT_XLSX_SAMPLE_PATH, index=False)\n",
    "print(\"OK:\", OUTPUT_XLSX_SAMPLE_PATH)\n",
    "\n",
    "print(\"== Terminé ==\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3550b96",
   "metadata": {},
   "source": [
    "## Stratégie d’estimation (niveau 1) : Probit et sélection de variables assistée par Machine Learning\n",
    "\n",
    "### Objectif du premier volet\n",
    "Le premier volet de ce travail vise à estimer un **modèle probit binaire** afin d’analyser les déterminants de la probabilité pour un individu d’être **travailleur transfrontalier dans la région Grand Est**. La variable dépendante prend la valeur 1 lorsque l’individu exerce une activité professionnelle à l’étranger tout en résidant dans le Grand Est, et 0 sinon.\n",
    "\n",
    "Le choix du probit s’inscrit dans un cadre économétrique standard pour les variables dichotomiques. Il permet une interprétation structurée des résultats, notamment à travers les **effets marginaux**, et constitue une référence largement utilisée dans l’analyse des comportements de mobilité du travail.\n",
    "\n",
    "### Pourquoi compléter le probit par une étape de Machine Learning ?\n",
    "La base de données finale résulte de l’appariement de sources INSEE détaillées (INDCVI et MOBPRO) et comprend un nombre important de variables individuelles, professionnelles, géographiques et liées au logement. Dans ce contexte, la sélection des covariables du modèle probit pose plusieurs enjeux :\n",
    "\n",
    "- limitation de l’**arbitraire** dans le choix des variables explicatives,\n",
    "- gestion de la **multicolinéarité** entre caractéristiques socio-démographiques et professionnelles,\n",
    "- identification de variables prédictives pertinentes dans un espace de dimension élevée.\n",
    "\n",
    "L’introduction d’une étape de Machine Learning ne vise pas à remplacer l’approche économétrique, mais à **sécuriser la spécification du modèle probit** en apportant une procédure de sélection systématique, reproductible et guidée par les données.\n",
    "\n",
    "### Méthode retenue : sélection pénalisée de type LASSO\n",
    "Une régression binaire pénalisée de type **LASSO (L1)** est utilisée comme outil de présélection des covariables. Cette méthode permet :\n",
    "\n",
    "- une **sélection automatique** des variables les plus informatives pour la prédiction du statut de travailleur transfrontalier,\n",
    "- la réduction de la dimension du modèle en mettant à zéro les coefficients des variables peu contributives,\n",
    "- une meilleure gestion des corrélations entre variables issues de sources riches et détaillées,\n",
    "- un calibrage rigoureux via **validation croisée**, limitant les risques de sur-ajustement.\n",
    "\n",
    "À ce stade exploratoire, l’utilisation d’un lien logit (logit-LASSO) est retenue pour des raisons pratiques et de robustesse numérique. La différence entre logit et probit portant essentiellement sur la forme de la fonction de lien, cette étape est considérée comme neutre du point de vue de la sélection des variables, l’objectif n’étant pas l’interprétation des coefficients mais l’identification des covariables informatives.\n",
    "\n",
    "### Enchaînement des étapes empiriques\n",
    "La stratégie d’estimation repose ainsi sur une approche en deux niveaux :\n",
    "\n",
    "1. **Sélection assistée par Machine Learning (outil auxiliaire)**  \n",
    "   - Estimation d’un modèle binaire pénalisé (LASSO) sur un échantillon de travail.\n",
    "   - Identification d’un ensemble restreint de variables candidates expliquant la probabilité d’être travailleur transfrontalier dans le Grand Est.\n",
    "\n",
    "2. **Estimation économétrique (modèle principal)**  \n",
    "   - Estimation d’un **probit standard** par maximum de vraisemblance sur la base finale.\n",
    "   - Analyse des coefficients et des effets marginaux afin de caractériser les déterminants individuels, professionnels et géographiques de la mobilité transfrontalière.\n",
    "\n",
    "### Positionnement méthodologique\n",
    "Cette approche permet de concilier :\n",
    "- la **rigueur interprétable** du modèle probit, au cœur de l’analyse économique,\n",
    "- et une sélection de variables **robuste et peu arbitraire**, issue d’outils de Machine Learning utilisés comme support méthodologique.\n",
    "\n",
    "L’étape de Machine Learning est ainsi mobilisée comme un **complément à l’économétrie**, destiné à renforcer la solidité et la crédibilité de l’analyse des travailleurs transfrontaliers dans la région Grand Est.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50a3555",
   "metadata": {},
   "source": [
    "## Choix de la méthode de pénalisation pour la sélection des variables\n",
    "\n",
    "Dans le cadre de l’estimation d’un modèle probit visant à analyser les déterminants du travail transfrontalier dans la région Grand Est, une étape préalable de sélection des covariables est nécessaire afin de construire une spécification parcimonieuse et interprétable.\n",
    "\n",
    "Plusieurs méthodes de pénalisation sont envisageables :\n",
    "\n",
    "- **Ridge (L2)** : cette méthode réduit la variance des estimateurs en pénalisant la taille des coefficients, mais **ne réalise pas de sélection explicite** des variables. L’ensemble des covariables demeure dans le modèle, ce qui ne répond pas à l’objectif de construction d’une spécification économétrique claire.\n",
    "\n",
    "- **Elastic Net (L1 + L2)** : cette approche combine les propriétés du LASSO et du Ridge et est particulièrement adaptée lorsque les variables explicatives sont fortement corrélées par groupes. Bien que pertinente, elle tend à conserver des ensembles de variables plus larges et conduit à des modèles moins parcimonieux.\n",
    "\n",
    "- **LASSO (L1)** : le LASSO permet une **sélection automatique et explicite** des variables en annulant les coefficients des covariables peu informatives. Il constitue ainsi un outil efficace de réduction de dimension et de présélection des variables pertinentes.\n",
    "\n",
    "Compte tenu de l’objectif principal — **construire un modèle probit interprétable pour l’analyse des travailleurs transfrontaliers du Grand Est** — le **LASSO est retenu** comme méthode de sélection des covariables. Cette approche offre un compromis adapté entre rigueur méthodologique, transparence de la spécification et facilité d’intégration dans un cadre économétrique standard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77459670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "PROFILING BDD_FINAL_new.csv\n",
      "==========================================================================================\n",
      "[1] Lecture : /Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/BDD_FINAL_new.csv\n",
      "OK: 494,483 lignes | 113 colonnes\n",
      "Aperçu colonnes (20 premières): ['COMMUNE', 'ARM', 'DCFLT', 'DCLT', 'AGEREVQ', 'GS', 'DEROU', 'DIPL', 'EMPL', 'ILT', 'ILTUU', 'IMMI', 'INATC', 'INEEM', 'INPOM', 'INPSM', 'IPONDI', 'LPRM', 'METRODOM', 'MOCO']\n",
      "\n",
      "[2] Construction du profil variables…\n",
      "OK: profil construit\n",
      "→ 113 variables profilées\n",
      "\n",
      "==========================================================================================\n",
      "RÉSUMÉS (console)\n",
      "==========================================================================================\n",
      "\n",
      "Top 15 variables avec le plus de manquants :\n",
      "   variable dtype_pandas type_simplifie  taux_manquants_%  nb_manquants\n",
      "        ARM      float64        numeric          100.0000        494483\n",
      "      DEROU       object    categorical           99.3745        491390\n",
      "      TYPMR      float64        numeric            0.6255          3093\n",
      "       LPRM      float64        numeric            0.6255          3093\n",
      "      NPERR      float64        numeric            0.6255          3093\n",
      "       TYPL      float64        numeric            0.6255          3093\n",
      "      STOCD      float64        numeric            0.6255          3093\n",
      "       VOIT      float64        numeric            0.6255          3093\n",
      "    AGEREVQ      float64        numeric            0.0002             1\n",
      "ID_INDIVIDU        int64        numeric            0.0000             0\n",
      "      NUMMI       object    categorical            0.0000             0\n",
      "     IPONDI      float64        numeric            0.0000             0\n",
      "       DCLT       object    categorical            0.0000             0\n",
      "    COMMUNE        int64        numeric            0.0000             0\n",
      "     DEPCOM        int64        numeric            0.0000             0\n",
      "\n",
      "Répartition par type_simplifie :\n",
      "type_simplifie\n",
      "categorical    63\n",
      "numeric        49\n",
      "bool            1\n",
      "\n",
      "Top 15 variables avec le plus de modalités/uniques :\n",
      "   variable type_simplifie  nb_modalites_ou_valeurs_uniques\n",
      "ID_INDIVIDU        numeric                           494483\n",
      "      NUMMI    categorical                            34696\n",
      "     IPONDI        numeric                            19749\n",
      "       DCLT    categorical                             6661\n",
      "    COMMUNE        numeric                             4646\n",
      "     DEPCOM        numeric                             4646\n",
      "       IRIS    categorical                             1057\n",
      "      DCFLT    categorical                              926\n",
      "     TRIRIS    categorical                              227\n",
      "  CANTVILLE        numeric                              176\n",
      "       DNAI    categorical                              105\n",
      "       ANAI        numeric                               89\n",
      "       AGED        numeric                               87\n",
      "     AGEREV        numeric                               86\n",
      "       NBPI    categorical                               21\n",
      "\n",
      "==========================================================================================\n",
      "EXPORTS\n",
      "==========================================================================================\n",
      "OK CSV : /Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/PROFILE_BDD_FINAL_new.csv\n",
      "OK XLSX: /Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/PROFILE_BDD_FINAL_new.xlsx\n",
      "\n",
      "==========================================================================================\n",
      "TERMINÉ\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROFILING COMPLET DE LA BDD (variables, types, manquants, modalités, stats num)\n",
    "- Lit BDD_FINAL_new.csv (séparateur ; )\n",
    "- Produit un tableau \"profile\" (1 ligne par variable)\n",
    "- Exporte un Excel avec 2 onglets :\n",
    "    1) profil_variables : complet\n",
    "    2) resume : plus lisible (colonnes clés + min/p50/max)\n",
    "- Affiche aussi des résumés utiles dans la console\n",
    "\n",
    "Auteur : Projet INSEE - Mobilité Transfrontalière (Mehdi)\n",
    "Date   : Janvier 2026\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import (\n",
    "    is_numeric_dtype,\n",
    "    is_bool_dtype,\n",
    "    is_datetime64_any_dtype\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# PARAMÈTRES (adapte si besoin)\n",
    "# =============================================================================\n",
    "INPUT_PATH = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/BDD_FINAL_new.csv\"\n",
    "\n",
    "OUT_PROFILE_XLSX = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/PROFILE_BDD_FINAL_new.xlsx\"\n",
    "OUT_PROFILE_CSV  = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/PROFILE_BDD_FINAL_new.csv\"\n",
    "\n",
    "TOP_K_MODALITES = 20          # top modalités pour catégorielles\n",
    "SAMPLE_ROWS_FOR_MODALITES = None  # ex: 200_000 si tu veux accélérer sur une grosse base (None = tout)\n",
    "\n",
    "# =============================================================================\n",
    "# UTILS\n",
    "# =============================================================================\n",
    "def safe_nanpercentile(arr, q):\n",
    "    \"\"\"\n",
    "    Percentiles robustes (gère tableaux vides / bool / non-num).\n",
    "    \"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    # si vide -> nan\n",
    "    if arr.size == 0:\n",
    "        return np.nan\n",
    "    # retire NaN\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    if arr.size == 0:\n",
    "        return np.nan\n",
    "    return float(np.nanpercentile(arr.astype(float), q))\n",
    "\n",
    "\n",
    "def simplify_type(series: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Renvoie un type simplifié lisible.\n",
    "    \"\"\"\n",
    "    if is_bool_dtype(series):\n",
    "        return \"bool\"\n",
    "    if is_datetime64_any_dtype(series):\n",
    "        return \"datetime\"\n",
    "    if is_numeric_dtype(series):\n",
    "        return \"numeric\"\n",
    "    return \"categorical\"\n",
    "\n",
    "\n",
    "def top_modalities(series: pd.Series, k=20) -> str:\n",
    "    \"\"\"\n",
    "    Renvoie une string \"modalite (count), ...\" pour les catégorielles.\n",
    "    Pour éviter l'explosion mémoire, on peut échantillonner.\n",
    "    \"\"\"\n",
    "    s = series\n",
    "\n",
    "    if SAMPLE_ROWS_FOR_MODALITES is not None and len(s) > SAMPLE_ROWS_FOR_MODALITES:\n",
    "        s = s.sample(SAMPLE_ROWS_FOR_MODALITES, random_state=42)\n",
    "\n",
    "    # Normalise un peu pour la lisibilité\n",
    "    # (on garde tel quel, juste strip pour string)\n",
    "    if s.dtype == \"object\" or str(s.dtype).startswith(\"string\"):\n",
    "        s = s.astype(\"string\").str.strip()\n",
    "\n",
    "    vc = s.value_counts(dropna=False).head(k)\n",
    "    parts = []\n",
    "    for idx, cnt in vc.items():\n",
    "        if pd.isna(idx):\n",
    "            label = \"NA\"\n",
    "        else:\n",
    "            label = str(idx)\n",
    "        parts.append(f\"{label} ({int(cnt)})\")\n",
    "    return \", \".join(parts)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) CHARGEMENT\n",
    "# =============================================================================\n",
    "print(\"=\" * 90)\n",
    "print(\"PROFILING BDD_FINAL_new.csv\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "if not os.path.exists(INPUT_PATH):\n",
    "    raise FileNotFoundError(f\"Fichier introuvable: {INPUT_PATH}\")\n",
    "\n",
    "print(f\"[1] Lecture : {INPUT_PATH}\")\n",
    "df = pd.read_csv(INPUT_PATH, sep=\";\", encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "print(f\"OK: {df.shape[0]:,} lignes | {df.shape[1]} colonnes\")\n",
    "print(\"Aperçu colonnes (20 premières):\", list(df.columns[:20]))\n",
    "\n",
    "# =============================================================================\n",
    "# 2) PROFILING COLONNE PAR COLONNE\n",
    "# =============================================================================\n",
    "print(\"\\n[2] Construction du profil variables…\")\n",
    "\n",
    "rows = []\n",
    "n = len(df)\n",
    "\n",
    "for col in df.columns:\n",
    "    s = df[col]\n",
    "    dtype = str(s.dtype)\n",
    "\n",
    "    # manquants\n",
    "    na_count = int(s.isna().sum())\n",
    "    na_pct = (na_count / n) * 100 if n > 0 else np.nan\n",
    "\n",
    "    # type simplifié\n",
    "    t_simpl = simplify_type(s)\n",
    "\n",
    "    # nb \"modalités\" / uniques (on garde la même colonne pour tout)\n",
    "    n_unique = int(s.nunique(dropna=True))\n",
    "\n",
    "    # stats numériques (si numérique ou bool)\n",
    "    stats = {\"min\": \"\", \"p1\": \"\", \"p50\": \"\", \"p99\": \"\", \"max\": \"\"}\n",
    "\n",
    "    if t_simpl in (\"numeric\", \"bool\"):\n",
    "        # IMPORTANT: booleans -> cast int, sinon numpy peut planter sur quantiles\n",
    "        if is_bool_dtype(s):\n",
    "            s_num = s.astype(\"Int64\").astype(float)\n",
    "        else:\n",
    "            s_num = pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "\n",
    "        arr = s_num.values\n",
    "        # retire NaN pour checks\n",
    "        arr_nonan = arr[np.isfinite(arr)]\n",
    "\n",
    "        if arr_nonan.size > 0:\n",
    "            stats = {\n",
    "                \"min\": float(np.nanmin(arr_nonan)),\n",
    "                \"p1\":  safe_nanpercentile(arr_nonan, 1),\n",
    "                \"p50\": safe_nanpercentile(arr_nonan, 50),\n",
    "                \"p99\": safe_nanpercentile(arr_nonan, 99),\n",
    "                \"max\": float(np.nanmax(arr_nonan)),\n",
    "            }\n",
    "        else:\n",
    "            stats = {\"min\": np.nan, \"p1\": np.nan, \"p50\": np.nan, \"p99\": np.nan, \"max\": np.nan}\n",
    "\n",
    "    # modalités principales (si catégorielle, sinon vide)\n",
    "    mod_princ = \"\"\n",
    "    if t_simpl == \"categorical\":\n",
    "        mod_princ = top_modalities(s, k=TOP_K_MODALITES)\n",
    "\n",
    "    rows.append({\n",
    "        \"variable\": col,\n",
    "        \"dtype_pandas\": dtype,\n",
    "        \"type_simplifie\": t_simpl,\n",
    "        \"nb_modalites_ou_valeurs_uniques\": n_unique,\n",
    "        \"taux_manquants_%\": round(na_pct, 4),\n",
    "        \"nb_manquants\": na_count,\n",
    "        \"modalites_principales\": mod_princ,\n",
    "        \"min\": stats[\"min\"],\n",
    "        \"p1\": stats[\"p1\"],\n",
    "        \"p50\": stats[\"p50\"],\n",
    "        \"p99\": stats[\"p99\"],\n",
    "        \"max\": stats[\"max\"],\n",
    "    })\n",
    "\n",
    "profile = pd.DataFrame(rows)\n",
    "\n",
    "# Tri utile: d’abord manquants décroissants, puis cardinalité décroissante\n",
    "profile = profile.sort_values(\n",
    "    by=[\"taux_manquants_%\", \"nb_modalites_ou_valeurs_uniques\"],\n",
    "    ascending=[False, False]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"OK: profil construit\")\n",
    "print(f\"→ {profile.shape[0]} variables profilées\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3) RÉSUMÉS CONSOLE (lisibles, pas de spam)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"RÉSUMÉS (console)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(\"\\nTop 15 variables avec le plus de manquants :\")\n",
    "print(\n",
    "    profile[[\"variable\", \"dtype_pandas\", \"type_simplifie\", \"taux_manquants_%\", \"nb_manquants\"]]\n",
    "    .head(15)\n",
    "    .to_string(index=False)\n",
    ")\n",
    "\n",
    "print(\"\\nRépartition par type_simplifie :\")\n",
    "print(profile[\"type_simplifie\"].value_counts().to_string())\n",
    "\n",
    "print(\"\\nTop 15 variables avec le plus de modalités/uniques :\")\n",
    "print(\n",
    "    profile[[\"variable\", \"type_simplifie\", \"nb_modalites_ou_valeurs_uniques\"]]\n",
    "    .sort_values(\"nb_modalites_ou_valeurs_uniques\", ascending=False)\n",
    "    .head(15)\n",
    "    .to_string(index=False)\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 4) EXPORTS (CSV + XLSX)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"EXPORTS\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# CSV\n",
    "profile.to_csv(OUT_PROFILE_CSV, index=False, sep=\";\")\n",
    "print(\"OK CSV :\", OUT_PROFILE_CSV)\n",
    "\n",
    "# Colonnes clés pour l’onglet \"resume\"\n",
    "cols_show = [\n",
    "    \"variable\",\n",
    "    \"dtype_pandas\",\n",
    "    \"type_simplifie\",\n",
    "    \"nb_modalites_ou_valeurs_uniques\",\n",
    "    \"taux_manquants_%\",\n",
    "    \"modalites_principales\",\n",
    "]\n",
    "\n",
    "# Sécurise si jamais une colonne change\n",
    "cols_resume = [c for c in cols_show if c in profile.columns] + [\"min\", \"p50\", \"max\"]\n",
    "\n",
    "with pd.ExcelWriter(OUT_PROFILE_XLSX, engine=\"openpyxl\") as writer:\n",
    "    profile.to_excel(writer, sheet_name=\"profil_variables\", index=False)\n",
    "    profile[cols_resume].to_excel(writer, sheet_name=\"resume\", index=False)\n",
    "\n",
    "print(\"OK XLSX:\", OUT_PROFILE_XLSX)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"TERMINÉ\")\n",
    "print(\"=\" * 90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f349e",
   "metadata": {},
   "source": [
    "## 4. Stratégie de sélection des variables par apprentissage automatique\n",
    "\n",
    "### 4.1. Objectif et positionnement méthodologique\n",
    "\n",
    "L’objectif de cette étape n’est pas la prédiction du statut de travailleur transfrontalier, mais la **réduction rigoureuse de la dimension de l’espace des covariables** en vue de l’estimation d’un modèle probit interprétable.\n",
    "\n",
    "L’apprentissage automatique est utilisé ici **exclusivement comme un outil de pré-sélection des variables explicatives**, et non comme un modèle structurel ou causal. Le modèle final retenu pour l’analyse économique reste un **probit binaire classique**, estimé sur un sous-ensemble de variables sélectionnées de manière systématique et reproductible.\n",
    "\n",
    "Cette approche s’inscrit dans la littérature récente combinant méthodes économétriques traditionnelles et techniques de régularisation, tout en respectant les exigences d’interprétabilité et de transparence propres à l’analyse micro-économétrique.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2. Population d’étude et variable dépendante\n",
    "\n",
    "La population étudiée est constituée des **actifs résidant dans la région Grand Est**, après appariement des bases INDCVI et MOBPRO. Le périmètre géographique est donc fixé ex ante, ce qui rend la variable REGION constante et exclue du jeu de covariables.\n",
    "\n",
    "La variable dépendante est définie comme un indicateur binaire du statut de travailleur transfrontalier :\n",
    "\n",
    "\\[\n",
    "Y_i = \\mathbb{1}(\\text{individu } i \\text{ exerce son activité professionnelle à l’étranger})\n",
    "\\]\n",
    "\n",
    "Ce statut est construit à partir des informations relatives au lieu de travail (codes DCFLT, DCLT, REGLT).\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3. Exclusion a priori des variables non admissibles (anti-leakage)\n",
    "\n",
    "Avant toute étape de sélection automatique, un filtrage strict des variables explicatives est opéré afin d’éviter toute fuite d’information (data leakage) ou toute identification artificielle du statut transfrontalier.\n",
    "\n",
    "Sont exclues a priori du jeu de covariables :\n",
    "\n",
    "- **Variables définissant directement la variable dépendante** :\n",
    "  - DCFLT, DCLT, REGLT\n",
    "  - ILT, ILTUU\n",
    "- **Identifiants et quasi-identifiants** :\n",
    "  - ID_INDIVIDU, NUMMI\n",
    "  - COMMUNE, DEPCOM, CANTVILLE\n",
    "  - IRIS et TRIRIS utilisés à un niveau trop fin\n",
    "- **Variables constantes ou dégénérées** :\n",
    "  - ARM (100 % manquante)\n",
    "  - DEROU (quasi intégralement manquante)\n",
    "  - METRODOM, RECH, REGION (constantes)\n",
    "- **Variables postérieures ou mécaniquement liées à l’activité transfrontalière** :\n",
    "  - Variables de localisation précise du lieu de travail\n",
    "  - Variables de transport (TRANS), exclues du modèle principal et réservées à des analyses complémentaires\n",
    "\n",
    "Ce filtrage garantit que l’ensemble des variables candidates est **prédéterminé** par rapport au statut transfrontalier et économiquement interprétable.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4. Structuration du jeu de covariables admissibles\n",
    "\n",
    "Les variables restantes sont organisées en blocs analytiques cohérents afin de guider l’étape de sélection et de faciliter l’interprétation économique :\n",
    "\n",
    "- **Démographie individuelle**  \n",
    "  (âge, sexe, origine, statut migratoire)\n",
    "- **Capital humain**  \n",
    "  (niveau de diplôme, catégorie socioprofessionnelle)\n",
    "- **Situation familiale et composition du ménage**  \n",
    "  (couple, structure familiale, nombre d’enfants, taille du ménage)\n",
    "- **Caractéristiques du logement et ancrage résidentiel**  \n",
    "  (type de logement, statut d’occupation, surface, ancienneté, équipement)\n",
    "- **Caractéristiques de l’emploi**  \n",
    "  (type de contrat, temps de travail, secteur d’activité)\n",
    "- **Contexte géographique agrégé**  \n",
    "  (département de résidence, catégorie d’IRIS, niveau d’appariement)\n",
    "\n",
    "Cette structuration permet de limiter les risques de sélection erratique et de faciliter les analyses de robustesse par blocs.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.5. Prétraitement des données avant sélection\n",
    "\n",
    "Avant l’estimation des modèles pénalisés, plusieurs traitements sont appliqués :\n",
    "\n",
    "- Regroupement des modalités rares afin d’éviter une fragmentation excessive de l’espace des covariables.\n",
    "- Traitement explicite des modalités de type *Z / ZZ* comme des modalités structurelles spécifiques.\n",
    "- Encodage des variables catégorielles par indicatrices (one-hot encoding).\n",
    "- Standardisation des variables numériques afin d’assurer la comparabilité des coefficients pénalisés.\n",
    "\n",
    "Ces étapes sont intégrées dans un pipeline reproductible afin de garantir la stabilité des résultats.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.6. Méthode de sélection par régression pénalisée\n",
    "\n",
    "La sélection des variables est réalisée à l’aide d’une **régression logistique pénalisée**, choisie pour sa stabilité numérique et sa compatibilité avec les procédures de validation croisée.\n",
    "\n",
    "Le modèle estimé est de la forme :\n",
    "\n",
    "\\[\n",
    "\\Pr(Y_i = 1 \\mid X_i) = \\Lambda(X_i'\\beta)\n",
    "\\]\n",
    "\n",
    "où \\(\\Lambda(\\cdot)\\) désigne la fonction logistique, et où le vecteur \\(\\beta\\) est estimé sous contrainte de pénalisation.\n",
    "\n",
    "Deux méthodes sont mobilisées :\n",
    "\n",
    "- **Elastic Net** (pénalisation mixte L1–L2) comme approche principale, adaptée à la présence de covariables corrélées.\n",
    "- **LASSO** (L1) comme test de robustesse.\n",
    "\n",
    "Les hyperparamètres sont sélectionnés par validation croisée (k-fold).\n",
    "\n",
    "---\n",
    "\n",
    "### 4.7. Sélection par stabilité\n",
    "\n",
    "Afin d’éviter une sélection dépendante d’un seul tirage de l’échantillon, une procédure de **stability selection** est mise en œuvre :\n",
    "\n",
    "- Estimation du modèle pénalisé sur plusieurs sous-échantillons ou plis de validation croisée.\n",
    "- Calcul de la fréquence de sélection de chaque variable.\n",
    "- Conservation des variables sélectionnées dans une proportion minimale des réplications (seuil fixé ex ante).\n",
    "\n",
    "Cette approche permet d’identifier un noyau de variables robustes et économiquement pertinentes.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.8. Estimation du modèle probit final\n",
    "\n",
    "Le sous-ensemble de variables sélectionnées est ensuite utilisé pour estimer un **modèle probit binaire standard** sur l’échantillon complet :\n",
    "\n",
    "\\[\n",
    "\\Pr(Y_i = 1 \\mid X_i) = \\Phi(X_i'\\gamma)\n",
    "\\]\n",
    "\n",
    "où \\(\\Phi(\\cdot)\\) désigne la fonction de répartition de la loi normale.\n",
    "\n",
    "L’interprétation repose sur les **effets marginaux moyens**, estimés avec des erreurs-types robustes. L’accent est mis sur les ordres de grandeur, les signes et la cohérence économique des effets, plutôt que sur la significativité statistique brute post-sélection.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.9. Analyses de robustesse\n",
    "\n",
    "Plusieurs vérifications sont menées afin d’évaluer la robustesse des résultats :\n",
    "\n",
    "- Comparaison des sélections Elastic Net et LASSO.\n",
    "- Ré-estimation du probit avec et sans certains blocs de variables (logement, famille).\n",
    "- Sensibilité à la pondération.\n",
    "- Estimations sur sous-échantillons.\n",
    "\n",
    "Ces tests permettent de s’assurer que les résultats ne reposent pas sur un choix arbitraire de spécification.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.10. Positionnement interprétatif\n",
    "\n",
    "Conformément aux bonnes pratiques, cette démarche ne vise pas à établir une relation causale stricte, mais à fournir une **analyse économétrique interprétable et robuste** des déterminants associés au statut de travailleur transfrontalier, en combinant rigueur économétrique et outils modernes de sélection de variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6d15131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Lecture: 494,483 lignes | 113 colonnes\n",
      "\n",
      "[OK] Export summary → /Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/DIAGNOSTICS_ML/DIAG_stepA_summary.csv\n",
      "[OK] Export flags   → /Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/DIAGNOSTICS_ML/DIAG_stepA_flags.csv\n",
      "[OK] Export checks  → /Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/DIAGNOSTICS_ML/DIAG_stepA_dataset_checks.txt\n",
      "\n",
      "=== QUICK PEEK: Top 15 missing_rate ===\n",
      "        variable    dtype  missing_rate  nunique_dropnaFalse  dominant_share  \\\n",
      "1            ARM  float64      1.000000                    1        1.000000   \n",
      "6          DEROU   object      0.993745                    2        0.993745   \n",
      "30         TYPMR  float64      0.006255                   10        0.548737   \n",
      "17          LPRM  float64      0.006255                    8        0.631555   \n",
      "21         NPERR  float64      0.006255                    7        0.294358   \n",
      "29          TYPL  float64      0.006255                    7        0.587745   \n",
      "26         STOCD  float64      0.006255                    6        0.605663   \n",
      "31          VOIT  float64      0.006255                    5        0.434535   \n",
      "4        AGEREVQ  float64      0.000002                   18        0.128662   \n",
      "106  ID_INDIVIDU    int64      0.000000               494483        0.000002   \n",
      "36         NUMMI   object      0.000000                34696        0.006255   \n",
      "16        IPONDI  float64      0.000000                19749        0.003668   \n",
      "3           DCLT   object      0.000000                 6661        0.089516   \n",
      "0        COMMUNE    int64      0.000000                 4646        0.066328   \n",
      "33        DEPCOM    int64      0.000000                 4646        0.066328   \n",
      "\n",
      "     suspect_name_by_pattern  \n",
      "1                          0  \n",
      "6                          0  \n",
      "30                         0  \n",
      "17                         0  \n",
      "21                         0  \n",
      "29                         0  \n",
      "26                         0  \n",
      "31                         0  \n",
      "4                          0  \n",
      "106                        0  \n",
      "36                         1  \n",
      "16                         0  \n",
      "3                          1  \n",
      "0                          1  \n",
      "33                         1  \n",
      "\n",
      "=== QUICK PEEK: Flags (first 40) ===\n",
      "           variable                                         reasons\n",
      "1               ARM                    HIGH_MISSING(>=95%);CONSTANT\n",
      "9         CANTVILLE                            SUSPECT_NAME_PATTERN\n",
      "12          CATIRIS                            SUSPECT_NAME_PATTERN\n",
      "0           COMMUNE                            SUSPECT_NAME_PATTERN\n",
      "2             DCFLT                            SUSPECT_NAME_PATTERN\n",
      "3              DCLT                            SUSPECT_NAME_PATTERN\n",
      "8            DEPCOM                            SUSPECT_NAME_PATTERN\n",
      "4             DEROU    HIGH_MISSING(>=95%);MANY_EMPTY_STRINGS(>=5%)\n",
      "17      ID_INDIVIDU                       HIGH_CARDINALITY(>=24725)\n",
      "13             IRIS                            SUSPECT_NAME_PATTERN\n",
      "5          METRODOM                                        CONSTANT\n",
      "11            NUMMI  HIGH_CARDINALITY(>=24725);SUSPECT_NAME_PATTERN\n",
      "14             RECH                                        CONSTANT\n",
      "6            REGION                                        CONSTANT\n",
      "7             REGLT                            SUSPECT_NAME_PATTERN\n",
      "15             TACT                                        CONSTANT\n",
      "16           TRIRIS                            SUSPECT_NAME_PATTERN\n",
      "20   WORK_ALLEMAGNE                            SUSPECT_NAME_PATTERN\n",
      "22    WORK_BELGIQUE                            SUSPECT_NAME_PATTERN\n",
      "18     WORK_IN_PAYS                            SUSPECT_NAME_PATTERN\n",
      "19  WORK_LUXEMBOURG                            SUSPECT_NAME_PATTERN\n",
      "21      WORK_SUISSE                            SUSPECT_NAME_PATTERN\n",
      "10    is_unique_key                                        CONSTANT\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CODE 1 — DIAGNOSTIC OBLIGATOIRE (ultra minimal & propre)\n",
    "# - Lit correctement le CSV (sep=';')\n",
    "# - Vérifie uniquement la cible TRANSFRONTALIER (exact match)\n",
    "# - Produit 3 fichiers : summary / flags / checks\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# Paramètres\n",
    "# ----------------------------\n",
    "IN_PATH = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/BDD_FINAL_new.csv\"\n",
    "OUT_DIR = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/DIAGNOSTICS_ML\"\n",
    "TARGET = \"TRANSFRONTALIER\"\n",
    "SEP = \";\"\n",
    "\n",
    "# Seuils\n",
    "TH_MISS_HIGH = 0.95\n",
    "TH_CONST_DOMINANT = 0.999\n",
    "TH_HIGH_CARD_ABS = 5000\n",
    "TH_HIGH_CARD_REL = 0.05\n",
    "TH_EMPTY_STR_HIGH = 0.05\n",
    "\n",
    "# Patterns “suspects” (leakage / ID / geo fine)\n",
    "SUSPECT_PATTERNS = [\n",
    "    r\"\\bwork\\b\", r\"pays\", r\"dcflt\", r\"dclt\", r\"reglt\",\n",
    "    r\"lux\", r\"suisse\", r\"allem\", r\"belg\", r\"etrang\",\n",
    "    r\"iris\", r\"triris\", r\"depc\", r\"commune\", r\"cantville\",\n",
    "    r\"\\bid\\b\", r\"nummi\", r\"ident\"\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# I/O checks + chargement\n",
    "# ----------------------------\n",
    "if not os.path.exists(IN_PATH):\n",
    "    raise FileNotFoundError(f\"Fichier introuvable: {IN_PATH}\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(IN_PATH, sep=SEP, low_memory=False)\n",
    "\n",
    "# Normalisation des noms (anti BOM / espaces)\n",
    "df.columns = (\n",
    "    pd.Index(df.columns)\n",
    "      .astype(str)\n",
    "      .str.replace(\"\\ufeff\", \"\", regex=False)\n",
    "      .str.strip()\n",
    ")\n",
    "\n",
    "n, p = df.shape\n",
    "print(f\"[OK] Lecture: {n:,} lignes | {p:,} colonnes\")\n",
    "\n",
    "if p == 1:\n",
    "    raise ValueError(\"Lecture en 1 seule colonne -> séparateur incorrect. Le fichier doit être en ';'.\")\n",
    "\n",
    "if TARGET not in df.columns:\n",
    "    raise ValueError(f\"Colonne cible '{TARGET}' absente. Colonnes dispo (extrait): {list(df.columns)[:30]} ...\")\n",
    "\n",
    "# ----------------------------\n",
    "# Checks dataset globaux\n",
    "# ----------------------------\n",
    "checks_lines = []\n",
    "checks_lines += [\n",
    "    f\"Source: {IN_PATH}\",\n",
    "    f\"sep: {SEP}\",\n",
    "    f\"Shape: {n} rows | {p} cols\",\n",
    "    f\"Target: {TARGET}\",\n",
    "]\n",
    "\n",
    "y = df[TARGET]\n",
    "checks_lines.append(f\"Target dtype: {y.dtype}\")\n",
    "checks_lines.append(f\"Target missing: {y.isna().sum()} ({y.isna().mean():.4%})\")\n",
    "\n",
    "y_num = pd.to_numeric(y, errors=\"coerce\")\n",
    "uniq = set(y_num.dropna().unique().tolist())\n",
    "checks_lines.append(f\"Target unique values (numeric coercion): {sorted(list(uniq))}\")\n",
    "\n",
    "if uniq.issubset({0, 1}):\n",
    "    checks_lines.append(f\"Target proportion of 1s: {float(y_num.mean()):.4%}\")\n",
    "else:\n",
    "    checks_lines.append(\"WARNING: Target is not strictly {0,1}.\")\n",
    "\n",
    "dup_rows = int(df.duplicated().sum())\n",
    "checks_lines.append(f\"Duplicate rows (exact duplicates): {dup_rows} ({dup_rows/n:.4%})\")\n",
    "\n",
    "# ----------------------------\n",
    "# Profil colonne par colonne\n",
    "# ----------------------------\n",
    "rows = []\n",
    "flags = []\n",
    "\n",
    "high_card_rel_abs = max(int(np.ceil(TH_HIGH_CARD_REL * n)), TH_HIGH_CARD_ABS)\n",
    "\n",
    "for col in df.columns:\n",
    "    s = df[col]\n",
    "    dtype = str(s.dtype)\n",
    "\n",
    "    n_missing = int(s.isna().sum())\n",
    "    miss_rate = float(n_missing / n)\n",
    "    nunique = int(s.nunique(dropna=False))\n",
    "\n",
    "    vc = s.value_counts(dropna=False)\n",
    "    dom_count = int(vc.iloc[0]) if len(vc) else 0\n",
    "    dom_share = float(dom_count / n) if n else np.nan\n",
    "    dom_value = vc.index[0] if len(vc) else np.nan\n",
    "\n",
    "    empty_share = np.nan\n",
    "    if s.dtype == \"object\":\n",
    "        empty_share = float((s.fillna(\"\").astype(str).str.len() == 0).mean())\n",
    "\n",
    "    inf_count = 0\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        inf_count = int(np.isinf(s.to_numpy()).sum())\n",
    "\n",
    "    col_l = col.lower()\n",
    "    suspect_name = any(re.search(pat, col_l) for pat in SUSPECT_PATTERNS)\n",
    "\n",
    "    rows.append({\n",
    "        \"variable\": col,\n",
    "        \"dtype\": dtype,\n",
    "        \"nunique_dropnaFalse\": nunique,\n",
    "        \"missing_rate\": miss_rate,\n",
    "        \"dominant_share\": dom_share,\n",
    "        \"dominant_value\": str(dom_value)[:120],\n",
    "        \"empty_string_share\": empty_share,\n",
    "        \"inf_count\": inf_count,\n",
    "        \"suspect_name_by_pattern\": int(suspect_name),\n",
    "        \"is_target\": int(col == TARGET),\n",
    "    })\n",
    "\n",
    "    reasons = []\n",
    "\n",
    "    if col == TARGET:\n",
    "        if miss_rate > 0:\n",
    "            reasons.append(\"TARGET_HAS_MISSING\")\n",
    "        if not uniq.issubset({0, 1}):\n",
    "            reasons.append(\"TARGET_NOT_BINARY_0_1\")\n",
    "    else:\n",
    "        if miss_rate >= TH_MISS_HIGH:\n",
    "            reasons.append(f\"HIGH_MISSING(>={TH_MISS_HIGH:.0%})\")\n",
    "        if nunique <= 1:\n",
    "            reasons.append(\"CONSTANT\")\n",
    "        elif dom_share >= TH_CONST_DOMINANT:\n",
    "            reasons.append(f\"QUASI_CONSTANT(dom>={TH_CONST_DOMINANT:.2%})\")\n",
    "        if nunique >= high_card_rel_abs:\n",
    "            reasons.append(f\"HIGH_CARDINALITY(>={high_card_rel_abs})\")\n",
    "        if not np.isnan(empty_share) and empty_share >= TH_EMPTY_STR_HIGH:\n",
    "            reasons.append(f\"MANY_EMPTY_STRINGS(>={TH_EMPTY_STR_HIGH:.0%})\")\n",
    "        if inf_count > 0:\n",
    "            reasons.append(\"HAS_INF_VALUES\")\n",
    "        if suspect_name:\n",
    "            reasons.append(\"SUSPECT_NAME_PATTERN\")\n",
    "\n",
    "    if reasons:\n",
    "        flags.append({\"variable\": col, \"reasons\": \";\".join(reasons)})\n",
    "\n",
    "summary_df = pd.DataFrame(rows).sort_values([\"missing_rate\", \"nunique_dropnaFalse\"], ascending=[False, False])\n",
    "flags_df = pd.DataFrame(flags).sort_values(\"variable\")\n",
    "\n",
    "# ----------------------------\n",
    "# Exports\n",
    "# ----------------------------\n",
    "summary_path = os.path.join(OUT_DIR, \"DIAG_stepA_summary.csv\")\n",
    "flags_path = os.path.join(OUT_DIR, \"DIAG_stepA_flags.csv\")\n",
    "checks_path = os.path.join(OUT_DIR, \"DIAG_stepA_dataset_checks.txt\")\n",
    "\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "flags_df.to_csv(flags_path, index=False)\n",
    "\n",
    "with open(checks_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(checks_lines))\n",
    "\n",
    "print(f\"\\n[OK] Export summary → {summary_path}\")\n",
    "print(f\"[OK] Export flags   → {flags_path}\")\n",
    "print(f\"[OK] Export checks  → {checks_path}\")\n",
    "\n",
    "print(\"\\n=== QUICK PEEK: Top 15 missing_rate ===\")\n",
    "print(summary_df[[\"variable\",\"dtype\",\"missing_rate\",\"nunique_dropnaFalse\",\"dominant_share\",\"suspect_name_by_pattern\"]].head(15))\n",
    "\n",
    "print(\"\\n=== QUICK PEEK: Flags (first 40) ===\")\n",
    "print(flags_df.head(40))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c932858b",
   "metadata": {},
   "source": [
    "## Conclusion du diagnostic préalable (Code 1) et implications pour la préparation ML\n",
    "\n",
    "Le Code 1 a permis de réaliser un diagnostic exhaustif de la base de données utilisée pour l’analyse des déterminants du travail transfrontalier. Cette étape est indispensable afin de garantir la qualité statistique, la cohérence économétrique et la validité méthodologique des modèles estimés ultérieurement.\n",
    "\n",
    "### 1. Validation globale de la base de données\n",
    "\n",
    "La base finale comprend **494 483 observations et 113 variables**, ce qui constitue un échantillon de grande taille, adapté à des méthodes de sélection de variables par apprentissage automatique et à l’estimation de modèles non linéaires (probit).\n",
    "\n",
    "La variable cible **`TRANSFRONTALIER`** :\n",
    "- est bien présente dans la base,\n",
    "- ne présente pas de valeurs manquantes,\n",
    "- est strictement binaire (0/1),\n",
    "- présente une distribution compatible avec une modélisation probabiliste.\n",
    "\n",
    "Aucune incohérence majeure (duplicats massifs, erreurs de lecture, corruption de colonnes) n’a été détectée au niveau global du jeu de données.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Identification des variables non exploitables en l’état\n",
    "\n",
    "Le diagnostic met en évidence plusieurs catégories de variables problématiques pour une utilisation directe dans un cadre ML / économétrique.\n",
    "\n",
    "#### 2.1 Variables à taux de valeurs manquantes extrême ou constantes\n",
    "\n",
    "Certaines variables présentent :\n",
    "- un taux de valeurs manquantes supérieur à 95 %,\n",
    "- ou une variance nulle (variables constantes).\n",
    "\n",
    "Exemples :\n",
    "- `ARM`\n",
    "- `DEROU`\n",
    "- `METRODOM`\n",
    "- `RECH`\n",
    "- `REGION`\n",
    "- `TACT`\n",
    "- `is_unique_key`\n",
    "\n",
    "Ces variables **n’apportent aucune information exploitable** et seront **exclues définitivement** de la base ML.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2 Variables à très forte cardinalité (identifiants techniques)\n",
    "\n",
    "Certaines variables présentent une cardinalité quasi unique, les rendant impropres à toute modélisation explicative ou prédictive standard :\n",
    "\n",
    "- `ID_INDIVIDU`\n",
    "- `NUMMI`\n",
    "\n",
    "Ces variables sont assimilables à des **identifiants techniques** et seront **exclues** de la base ML afin d’éviter toute sur-adaptation ou fuite d’information.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Variables géographiques fines et variables potentiellement fuyantes\n",
    "\n",
    "Le diagnostic a identifié plusieurs variables marquées comme *suspectes* au sens méthodologique, car elles décrivent soit :\n",
    "- des localisations géographiques très fines,\n",
    "- soit des informations directement liées au lieu de travail à l’étranger.\n",
    "\n",
    "Cela concerne notamment :\n",
    "- `COMMUNE`, `DEPCOM`, `CANTVILLE`, `IRIS`, `TRIRIS`, `CATIRIS`\n",
    "- `DCLT`, `DCFLT`, `REGLT`\n",
    "- les variables `WORK_*` (`WORK_ALLEMAGNE`, `WORK_SUISSE`, `WORK_LUXEMBOURG`, `WORK_BELGIQUE`, `WORK_IN_PAYS`)\n",
    "\n",
    "Ces variables ne sont **pas erronées**, mais leur utilisation nécessite un **choix économétrique explicite**, car elles sont fortement corrélées à la variable cible.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Positionnement méthodologique retenu pour la suite\n",
    "\n",
    "Conformément à l’objectif de l’étude, une distinction claire est opérée entre :\n",
    "\n",
    "- **Étape de sélection des variables (ML)**  \n",
    "- **Étape d’estimation du modèle final (probit)**\n",
    "\n",
    "#### 4.1 Pour la sélection ML (Code 2)\n",
    "\n",
    "Le ML est utilisé **uniquement comme outil de sélection de variables pertinentes**, et non comme modèle final.\n",
    "\n",
    "À ce stade :\n",
    "- les variables constantes, quasi constantes ou à très forte cardinalité sont exclues,\n",
    "- les identifiants techniques sont exclus,\n",
    "- les variables explicitement post-déterminées par le statut transfrontalier (notamment `WORK_*`) sont **exclues du processus de sélection ML**, afin d’éviter toute domination mécanique de la sélection et toute fuite d’information.\n",
    "\n",
    "Le ML opère donc sur un ensemble de variables **ex ante**, décrivant les caractéristiques individuelles, familiales, professionnelles et résidentielles des individus.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4.2 Pour le modèle probit final\n",
    "\n",
    "Le modèle probit a un objectif **prédictif**, mais également **analytique**. À ce titre :\n",
    "\n",
    "- les variables sélectionnées par le ML constituent le socle du modèle principal,\n",
    "- les variables `WORK_*`, qui capturent des effets structurels liés au pays de travail (attractivité économique, fiscale, institutionnelle), pourront être **réintroduites de manière contrôlée** dans le modèle probit ou mobilisées dans des extensions (analyses conditionnelles, modèles par pays, modèles multinomiaux).\n",
    "\n",
    "Ce choix permet de préserver :\n",
    "- la cohérence économétrique,\n",
    "- l’interprétabilité des coefficients,\n",
    "- et la robustesse des résultats.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Implications pour le Code 2 – Préparation de la base ML\n",
    "\n",
    "Le Code 2 consistera donc à :\n",
    "\n",
    "1. Construire une base ML nettoyée :\n",
    "   - exclusion des variables non informatives,\n",
    "   - exclusion des identifiants et variables constantes,\n",
    "   - séparation explicite des variables réservées aux extensions.\n",
    "\n",
    "2. Encoder correctement les variables catégorielles retenues.\n",
    "\n",
    "3. Préparer une matrice de données strictement adaptée à la sélection de variables par méthodes pénalisées (LASSO / Elastic Net).\n",
    "\n",
    "Cette préparation repose directement sur les résultats du diagnostic du Code 1 et garantit la solidité méthodologique de l’ensemble de l’analyse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73895b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Lecture : 494,483 lignes | 113 colonnes\n",
      "[OK] Features candidates conservées : 87\n",
      "[OK] Matrice ML finale : 494,483 x 452\n",
      "[OK] Exports terminés – base prête pour Elastic Net\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CODE 2 — PRÉPARATION DE LA BDD ML (VERSION CORRIGÉE ET COHÉRENTE)\n",
    "# Objectif : créer une base \"prête ML\" UNIQUEMENT pour la\n",
    "# SÉLECTION DE VARIABLES (Elastic Net / Lasso)\n",
    "#\n",
    "# PRINCIPES STRICTS :\n",
    "# 1) Exclusion définitive :\n",
    "#    - variables constantes / quasi-nulles\n",
    "#    - identifiants\n",
    "#    - variables d’ingénierie / appariement\n",
    "#\n",
    "# 2) Exclusion TEMPORAIRE (phase ML uniquement) :\n",
    "#    - variables de leakage (lieu/pays de travail)\n",
    "#\n",
    "# 3) Exclusion définitive :\n",
    "#    - géographie fine ou proxy géographique\n",
    "#\n",
    "# 4) AUCUN choix économique ici :\n",
    "#    - les arbitrages se feront PLUS TARD via Elastic Net\n",
    "#\n",
    "# Sorties :\n",
    "#  - X_ml_ready.csv\n",
    "#  - y.csv\n",
    "#  - dropped_columns.csv\n",
    "#  - kept_raw_features.csv\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Paramètres\n",
    "# ----------------------------\n",
    "IN_PATH = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/BDD_FINAL_new.csv\"\n",
    "OUT_DIR = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/ML_READY\"\n",
    "\n",
    "SEP = \";\"\n",
    "TARGET = \"TRANSFRONTALIER\"\n",
    "WEIGHT = \"IPONDI\"   # conservé à part, jamais feature ML\n",
    "\n",
    "TOP_K_LEVELS = 50\n",
    "MIN_FREQ = 200\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Chargement\n",
    "# ----------------------------\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(IN_PATH, sep=SEP, low_memory=False)\n",
    "df.columns = (\n",
    "    df.columns.astype(str)\n",
    "    .str.replace(\"\\ufeff\", \"\", regex=False)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "n, p = df.shape\n",
    "print(f\"[OK] Lecture : {n:,} lignes | {p:,} colonnes\")\n",
    "\n",
    "if TARGET not in df.columns:\n",
    "    raise ValueError(f\"Colonne cible '{TARGET}' absente.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) LISTES D’EXCLUSION — VALIDÉES MÉTHODOLOGIQUEMENT\n",
    "# ----------------------------\n",
    "\n",
    "# A. Variables NON INFORMATIVES / TECHNIQUES (EXCLUSION DÉFINITIVE)\n",
    "DROP_ALWAYS = [\n",
    "    \"ARM\", \"DEROU\", \"METRODOM\", \"RECH\", \"REGION\", \"TACT\", \"is_unique_key\"\n",
    "]\n",
    "\n",
    "# B. Identifiants\n",
    "DROP_IDENTIFIERS = [\n",
    "    \"ID_INDIVIDU\", \"NUMMI\"\n",
    "]\n",
    "\n",
    "# C. Variables d’INGÉNIERIE / APPARIEMENT (ERREUR MAJEURE SI CONSERVÉES)\n",
    "DROP_ENGINEERING = [\n",
    "    \"COMMUNE\", \"DEPCOM\", \"IRIS\", \"TRIRIS\",\n",
    "    \"CANTVILLE\", \"CATIRIS\", \"NIVEAU_APPARIEMENT\"\n",
    "]\n",
    "\n",
    "# D. Variables de LEAKAGE (exclues POUR LA PHASE ML SEULEMENT)\n",
    "DROP_LEAKAGE_FOR_SELECTION = [\n",
    "    \"WORK_IN_PAYS\", \"WORK_LUXEMBOURG\", \"WORK_ALLEMAGNE\",\n",
    "    \"WORK_SUISSE\", \"WORK_BELGIQUE\",\n",
    "    \"DCLT\", \"DCFLT\", \"REGLT\"\n",
    "]\n",
    "\n",
    "# E. Poids (jamais feature ML)\n",
    "DROP_WEIGHT = [WEIGHT] if WEIGHT in df.columns else []\n",
    "\n",
    "# Ensemble final des exclusions\n",
    "DROP_ALL = set(\n",
    "    DROP_ALWAYS\n",
    "    + DROP_IDENTIFIERS\n",
    "    + DROP_ENGINEERING\n",
    "    + DROP_LEAKAGE_FOR_SELECTION\n",
    "    + DROP_WEIGHT\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Séparation cible / features\n",
    "# ----------------------------\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "X_raw = df.drop(\n",
    "    columns=[c for c in DROP_ALL if c in df.columns] + [TARGET],\n",
    "    errors=\"ignore\"\n",
    ")\n",
    "\n",
    "print(f\"[OK] Features candidates conservées : {X_raw.shape[1]}\")\n",
    "\n",
    "# Traçabilité\n",
    "dropped_effective = sorted([c for c in df.columns if c not in X_raw.columns and c != TARGET])\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Préparation des variables\n",
    "# ----------------------------\n",
    "def collapse_rare_levels(s, top_k=TOP_K_LEVELS, min_freq=MIN_FREQ):\n",
    "    s = s.astype(\"object\").where(~s.isna(), \"__MISSING__\")\n",
    "    vc = s.value_counts()\n",
    "    keep = set(vc.head(top_k).index)\n",
    "    keep |= set(vc[vc >= min_freq].index)\n",
    "    return s.where(s.isin(keep), \"__OTHER__\")\n",
    "\n",
    "# Séparation types\n",
    "cat_cols = [c for c in X_raw.columns if X_raw[c].dtype == \"object\"]\n",
    "num_cols = [c for c in X_raw.columns if c not in cat_cols]\n",
    "\n",
    "# A. Catégorielles\n",
    "X_cat = pd.DataFrame(index=X_raw.index)\n",
    "cat_info = []\n",
    "\n",
    "for c in cat_cols:\n",
    "    s = collapse_rare_levels(X_raw[c])\n",
    "    X_cat[c] = s\n",
    "    cat_info.append({\n",
    "        \"variable\": c,\n",
    "        \"n_modalites_post\": int(s.nunique())\n",
    "    })\n",
    "\n",
    "# B. Numériques (imputation médiane)\n",
    "X_num = pd.DataFrame(index=X_raw.index)\n",
    "num_info = []\n",
    "\n",
    "for c in num_cols:\n",
    "    s = pd.to_numeric(X_raw[c], errors=\"coerce\")\n",
    "    med = float(s.median())\n",
    "    X_num[c] = s.fillna(med)\n",
    "    num_info.append({\n",
    "        \"variable\": c,\n",
    "        \"imputation\": \"median\",\n",
    "        \"median\": med\n",
    "    })\n",
    "\n",
    "# ----------------------------\n",
    "# 5) One-Hot Encoding\n",
    "# ----------------------------\n",
    "X_cat_dum = pd.get_dummies(X_cat, drop_first=False)\n",
    "X = pd.concat([X_num, X_cat_dum], axis=1)\n",
    "\n",
    "# Sécurité finale\n",
    "if X.isna().sum().sum() > 0:\n",
    "    raise ValueError(\"Il reste des NaN dans X après préparation.\")\n",
    "\n",
    "print(f\"[OK] Matrice ML finale : {X.shape[0]:,} x {X.shape[1]:,}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Exports\n",
    "# ----------------------------\n",
    "X.to_csv(os.path.join(OUT_DIR, \"X_ml_ready.csv\"), index=False)\n",
    "y.to_csv(os.path.join(OUT_DIR, \"y.csv\"), index=False)\n",
    "\n",
    "pd.DataFrame({\"variable\": dropped_effective}).to_csv(\n",
    "    os.path.join(OUT_DIR, \"dropped_columns.csv\"), index=False\n",
    ")\n",
    "\n",
    "pd.DataFrame({\"variable\": X_raw.columns}).to_csv(\n",
    "    os.path.join(OUT_DIR, \"kept_raw_features.csv\"), index=False\n",
    ")\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"categorical_info.json\"), \"w\") as f:\n",
    "    json.dump(cat_info, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"numeric_info.json\"), \"w\") as f:\n",
    "    json.dump(num_info, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"[OK] Exports terminés – base prête pour Elastic Net\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f8f2c",
   "metadata": {},
   "source": [
    "# Arbitrages conceptuels des variables explicatives\n",
    "\n",
    "Dans la base issue de l’appariement des fichiers INDCVI et MOBPRO, plusieurs dimensions\n",
    "socio-économiques sont décrites par des ensembles de variables alternatives ou\n",
    "hiérarchiquement liées.  \n",
    "Afin d’éviter toute redondance conceptuelle, instabilité statistique et perte\n",
    "d’interprétabilité, des arbitrages économiquement motivés ont été opérés **en amont**\n",
    "de toute procédure de sélection automatique par machine learning.\n",
    "\n",
    "Le principe général retenu est le suivant :\n",
    "\n",
    "> Pour une même dimension conceptuelle, nous conservons les variables apportant  \n",
    "> (i) la plus forte granularité informationnelle utile,  \n",
    "> (ii) une interprétabilité économétrique claire,  \n",
    "> (iii) une compatibilité avec un modèle probit explicatif.\n",
    "\n",
    "Les arbitrages finaux sont détaillés ci-dessous.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Âge\n",
    "\n",
    "Variables candidates :\n",
    "- AGED\n",
    "- AGEREV\n",
    "- AGEREVQ\n",
    "- AGER20\n",
    "- ANAI\n",
    "\n",
    "Choix retenu :\n",
    "- **AGED**\n",
    "\n",
    "Justification :\n",
    "L’âge exact constitue une variable continue permettant d’exploiter l’ensemble\n",
    "de l’information disponible. Dans un cadre probit, il autorise l’introduction\n",
    "éventuelle de non-linéarités (termes quadratiques ou splines) sans perte de précision,\n",
    "contrairement aux classes d’âge qui introduisent des effets de seuil arbitraires.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Statut d’activité et position socio-professionnelle\n",
    "\n",
    "Variables candidates :\n",
    "- STAT, STATR\n",
    "- EMPL\n",
    "- PCSL\n",
    "- GS, STAT_GSEC\n",
    "\n",
    "Choix retenus :\n",
    "- **STATR** (statut d’activité détaillé)\n",
    "- **PCSL** (position socio-professionnelle)\n",
    "- **GS** (statut public / privé)\n",
    "\n",
    "Justification :\n",
    "Le statut d’activité, la position socio-professionnelle et le statut institutionnel\n",
    "capturent des dimensions distinctes du marché du travail. Les conserver conjointement\n",
    "permet de différencier l’effet de l’insertion sur le marché du travail, de la\n",
    "stratification sociale et du cadre institutionnel de l’emploi.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Mobilité domicile–travail\n",
    "\n",
    "Variables candidates :\n",
    "- ILT\n",
    "- ILTUU\n",
    "- TRANS\n",
    "\n",
    "Choix retenus :\n",
    "- **ILT**\n",
    "- **TRANS**\n",
    "\n",
    "Justification :\n",
    "ILT mesure l’intensité ou la distance des déplacements domicile–travail et constitue\n",
    "une variable quantitative particulièrement informative. TRANS capte le mode de\n",
    "transport, apportant une information complémentaire sur les contraintes et choix\n",
    "de mobilité. ILTUU est dominée par ILT et n’est pas conservée.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Taille et structure du ménage\n",
    "\n",
    "Variables candidates :\n",
    "- NPERR\n",
    "- INPER, INPERF\n",
    "- INFAM\n",
    "- INPSM\n",
    "- INEEM\n",
    "\n",
    "Choix retenus :\n",
    "- **NPERR**\n",
    "- **INFAM**\n",
    "\n",
    "Justification :\n",
    "NPERR fournit la mesure la plus granulaire de la taille du ménage. INFAM permet de\n",
    "distinguer la structure familiale (famille / non-famille), dimension complémentaire\n",
    "non redondante avec la taille.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Présence d’enfants\n",
    "\n",
    "Variables candidates :\n",
    "- NENFR\n",
    "- NE3FR\n",
    "- NE5FR\n",
    "- NE17FR\n",
    "- NE24FR\n",
    "\n",
    "Choix retenu :\n",
    "- **NE17FR**\n",
    "\n",
    "Justification :\n",
    "NE17FR identifie la présence et le nombre d’enfants à charge dans une tranche d’âge\n",
    "économiquement pertinente, directement liée aux contraintes de mobilité et aux\n",
    "arbitrages professionnels.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Typologie du ménage\n",
    "\n",
    "Variables candidates :\n",
    "- MOCO\n",
    "- MOCO_DET\n",
    "- TYPMC\n",
    "- TYPMR\n",
    "- TDM8\n",
    "- SFM\n",
    "\n",
    "Choix retenus :\n",
    "- **TDM8**\n",
    "- **SFM**\n",
    "\n",
    "Justification :\n",
    "TDM8 offre une typologie synthétique et robuste des ménages, limitant la fragmentation\n",
    "excessive des modalités. SFM complète cette information en distinguant les formes\n",
    "familiales essentielles, sans introduire de bruit catégoriel inutile.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Conditions de logement\n",
    "\n",
    "Variables candidates :\n",
    "- SURF\n",
    "- HLML\n",
    "- GARL\n",
    "- BAIN, WC, CHAU, CLIM, CUIS, EAU, EGOUL, ELEC\n",
    "- BATI, CATL, CATPC\n",
    "\n",
    "Choix retenus :\n",
    "- **SURF**\n",
    "- **HLML**\n",
    "- **CATPC**\n",
    "\n",
    "Justification :\n",
    "SURF capte la surface du logement et constitue un proxy robuste du niveau de vie.\n",
    "HLML distingue le logement social, fortement corrélé à la position socio-économique.\n",
    "CATPC synthétise le type de logement de manière plus informative que les indicateurs\n",
    "binaires très faiblement discriminants.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Origine et migration\n",
    "\n",
    "Variables candidates :\n",
    "- IMMI\n",
    "- INATC\n",
    "- IRAN\n",
    "- DNAI\n",
    "- ORIDT\n",
    "\n",
    "Choix retenus :\n",
    "- **IMMI**\n",
    "- **INATC**\n",
    "\n",
    "Justification :\n",
    "IMMI identifie le statut d’immigré, tandis que INATC renseigne la nationalité actuelle.\n",
    "Les autres variables sont trop fines ou indirectes et n’apportent pas d’information\n",
    "supplémentaire robuste pour l’analyse.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Formation et éducation\n",
    "\n",
    "Variables candidates :\n",
    "- DIPL\n",
    "- ETUD\n",
    "- ILETUD\n",
    "\n",
    "Choix retenus :\n",
    "- **DIPL**\n",
    "- **ETUD**\n",
    "\n",
    "Justification :\n",
    "DIPL mesure le niveau de diplôme atteint, variable clé pour l’accès au marché du\n",
    "travail transfrontalier. ETUD permet d’identifier les individus encore en formation.\n",
    "ILETUD est redondante et n’est pas conservée.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Secteur d’activité\n",
    "\n",
    "Variables candidates :\n",
    "- NA5\n",
    "- NA17\n",
    "\n",
    "Choix retenu :\n",
    "- **NA17**\n",
    "\n",
    "Justification :\n",
    "NA17 fournit une désagrégation sectorielle plus fine, permettant de mieux capter\n",
    "les effets sectoriels spécifiques sur la probabilité de travail transfrontalier.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Santé\n",
    "\n",
    "Variables candidates :\n",
    "- SANI\n",
    "- SANIDOM\n",
    "\n",
    "Choix retenu :\n",
    "- **SANI**\n",
    "\n",
    "Justification :\n",
    "SANI décrit l’état de santé individuel, susceptible d’influencer directement la\n",
    "mobilité professionnelle. SANIDOM est liée au logement et déjà couverte par d’autres\n",
    "variables retenues.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Les arbitrages opérés permettent de constituer une base explicative cohérente,\n",
    "économiquement fondée et interprétable.  \n",
    "La sélection automatique par machine learning (Elastic Net) intervient uniquement\n",
    "en aval, pour gérer la colinéarité résiduelle et stabiliser l’estimation, sans se\n",
    "substituer aux choix conceptuels de l’économiste.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a88fb654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "             CODE 3 — FILTRAGE POST-ARBITRAGES ÉCONOMIQUES            \n",
      "======================================================================\n",
      "\n",
      "📅 Exécution : 2026-01-06 18:40:42\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "  1. CHARGEMENT DES DONNÉES\n",
      "──────────────────────────────────────────────────\n",
      "  📂 Répertoire source : /Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/ML_READY\n",
      "  📄 Fichier X : X_ml_ready.csv\n",
      "  📄 Fichier y : y.csv\n",
      "\n",
      "  ⏳ Chargement en cours...\n",
      "\n",
      "  ✅ X chargé avec succès\n",
      "     • Observations : 494,483\n",
      "     • Colonnes (features) : 452\n",
      "     • Taille mémoire : 332.0 MB\n",
      "\n",
      "  ✅ y chargé avec succès\n",
      "     • Observations : 494,483\n",
      "     • Colonnes : 1\n",
      "\n",
      "  📊 Distribution de la variable cible :\n",
      "     • 0 : 450,219 (91.05%)\n",
      "     • 1 : 44,264 (8.95%)\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "  2. MAPPING VARIABLES CONCEPTUELLES → COLONNES\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "  Variables conceptuelles demandées : 20\n",
      "  Colonnes disponibles dans X : 452\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "  3. DÉTAIL PAR GROUPE THÉMATIQUE\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "  📁 Âge\n",
      "     ✓ AGED         → numérique (1 col)\n",
      "     ──────────────────────────────\n",
      "     Sous-total groupe : 1 colonnes\n",
      "\n",
      "  📁 Statut d'activité / PCS\n",
      "     ✓ STATR        → numérique (1 col)\n",
      "     ✓ PCSL         → catégorielle (17 dummies)\n",
      "     ✓ GS           → numérique (1 col)\n",
      "     ──────────────────────────────\n",
      "     Sous-total groupe : 19 colonnes\n",
      "\n",
      "  📁 Mobilité domicile-travail\n",
      "     ✓ ILT          → numérique (1 col)\n",
      "     ✓ TRANS        → numérique (1 col)\n",
      "     ──────────────────────────────\n",
      "     Sous-total groupe : 2 colonnes\n",
      "\n",
      "  📁 Taille/structure ménage\n",
      "     ✓ NPERR        → numérique (1 col)\n",
      "     ✓ INFAM        → catégorielle (4 dummies)\n",
      "     ──────────────────────────────\n",
      "     Sous-total groupe : 5 colonnes\n",
      "\n",
      "  📁 Présence d'enfants\n",
      "     ✓ NE17FR       → catégorielle (6 dummies)\n",
      "     ──────────────────────────────\n",
      "     Sous-total groupe : 6 colonnes\n",
      "\n",
      "  📁 Typologie ménage\n",
      "     ✓ TDM8         → catégorielle (9 dummies)\n",
      "     ✓ SFM          → catégorielle (18 dummies)\n",
      "     ──────────────────────────────\n",
      "     Sous-total groupe : 27 colonnes\n",
      "\n",
      "  📁 Logement\n",
      "     ✓ SURF         → catégorielle (8 dummies)\n",
      "     ✓ HLML         → catégorielle (3 dummies)\n",
      "     ✓ CATPC        → numérique (1 col)\n",
      "     ──────────────────────────────\n",
      "     Sous-total groupe : 12 colonnes\n",
      "\n",
      "  📁 Origine / migration\n",
      "     ✓ IMMI         → numérique (1 col)\n",
      "     ✓ INATC        → numérique (1 col)\n",
      "     ──────────────────────────────\n",
      "     Sous-total groupe : 2 colonnes\n",
      "\n",
      "  📁 Formation / éducation\n",
      "     ✓ DIPL         → numérique (1 col)\n",
      "     ✓ ETUD         → numérique (1 col)\n",
      "     ──────────────────────────────\n",
      "     Sous-total groupe : 2 colonnes\n",
      "\n",
      "  📁 Secteur d'activité\n",
      "     ✓ NA17         → catégorielle (17 dummies)\n",
      "     ──────────────────────────────\n",
      "     Sous-total groupe : 17 colonnes\n",
      "\n",
      "  📁 Santé\n",
      "     ✓ SANI         → catégorielle (4 dummies)\n",
      "     ──────────────────────────────\n",
      "     Sous-total groupe : 4 colonnes\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "  4. STATISTIQUES GLOBALES DE SÉLECTION\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "  📊 Résumé du mapping :\n",
      "     • Variables conceptuelles trouvées : 20/20\n",
      "     • Variables conceptuelles manquantes : 0\n",
      "\n",
      "  📐 Dimensions :\n",
      "     • Variables numériques : 11\n",
      "     • Variables catégorielles : 9\n",
      "     • Total dummies catégorielles : 86\n",
      "     • Total colonnes sélectionnées : 97\n",
      "\n",
      "  📉 Réduction de dimension :\n",
      "     • Colonnes initiales : 452\n",
      "     • Colonnes conservées : 97\n",
      "     • Colonnes supprimées : 355\n",
      "     • Taux de réduction : 78.5%\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "  5. CONSTRUCTION DE LA BASE FINALE\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "  📦 X_final créé :\n",
      "     • Shape : 494,483 × 97\n",
      "     • Taille mémoire : 82.1 MB\n",
      "\n",
      "  🔍 Vérification des valeurs manquantes :\n",
      "     ✅ Aucune valeur manquante détectée\n",
      "\n",
      "  📋 Types de données :\n",
      "     • bool: 86 colonnes\n",
      "     • int64: 10 colonnes\n",
      "     • float64: 1 colonnes\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "  6. EXPORT DES FICHIERS\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "  ⏳ Export en cours...\n",
      "\n",
      "  ✅ Fichiers exportés avec succès :\n",
      "     📄 X_final : /Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/ML_READY/FINAL_AFTER_ARBITRAGES/X_ml_final_arbitrages.csv\n",
      "        → Taille : 251.3 MB\n",
      "     📄 y_final : /Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/ML_READY/FINAL_AFTER_ARBITRAGES/y_final.csv\n",
      "        → Taille : 0.9 MB\n",
      "     📄 Rapport : /Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/ML_READY/FINAL_AFTER_ARBITRAGES/keep_features_report.txt\n",
      "\n",
      "======================================================================\n",
      "                             RÉSUMÉ FINAL                             \n",
      "======================================================================\n",
      "\n",
      "  ┌─────────────────────────────────────────────────────────┐\n",
      "  │  FILTRAGE POST-ARBITRAGES TERMINÉ AVEC SUCCÈS           │\n",
      "  ├─────────────────────────────────────────────────────────┤\n",
      "  │  Observations     :    494,483                       │\n",
      "  │  Features initiales:        452                       │\n",
      "  │  Features finales  :         97                       │\n",
      "  │  Réduction         :       78.5%                      │\n",
      "  ├─────────────────────────────────────────────────────────┤\n",
      "  │  Variables manquantes : 0                                │\n",
      "  └─────────────────────────────────────────────────────────┘\n",
      "\n",
      "  📋 Aperçu des colonnes conservées (10 premières) :\n",
      "      1. AGED\n",
      "      2. STATR\n",
      "      3. PCSL_11\n",
      "      4. PCSL_12\n",
      "      5. PCSL_21\n",
      "      6. PCSL_22\n",
      "      7. PCSL_23\n",
      "      8. PCSL_24\n",
      "      9. PCSL_31\n",
      "     10. PCSL_32\n",
      "     ... et 87 autres colonnes\n",
      "\n",
      "  📁 Fichiers disponibles dans :\n",
      "     /Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/ML_READY/FINAL_AFTER_ARBITRAGES\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CODE 3 — CONSTRUCTION DE LA BDD \"ARBITRAGES ÉCONOMIQUES\" (post CODE 2)\n",
    "# Version améliorée avec affichage détaillé dans la console\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Paramètres\n",
    "# ----------------------------\n",
    "ML_READY_DIR = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/ML_READY\"\n",
    "X_PATH = os.path.join(ML_READY_DIR, \"X_ml_ready.csv\")\n",
    "Y_PATH = os.path.join(ML_READY_DIR, \"y.csv\")\n",
    "\n",
    "OUT_DIR = os.path.join(ML_READY_DIR, \"FINAL_AFTER_ARBITRAGES\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_X = os.path.join(OUT_DIR, \"X_ml_final_arbitrages.csv\")\n",
    "OUT_Y = os.path.join(OUT_DIR, \"y_final.csv\")\n",
    "OUT_REPORT = os.path.join(OUT_DIR, \"keep_features_report.txt\")\n",
    "\n",
    "# ----------------------------\n",
    "# Fonctions utilitaires d'affichage\n",
    "# ----------------------------\n",
    "def print_header(title):\n",
    "    \"\"\"Affiche un en-tête formaté\"\"\"\n",
    "    width = 70\n",
    "    print(\"\\n\" + \"=\" * width)\n",
    "    print(f\" {title}\".center(width))\n",
    "    print(\"=\" * width)\n",
    "\n",
    "def print_section(title):\n",
    "    \"\"\"Affiche un titre de section\"\"\"\n",
    "    print(f\"\\n{'─' * 50}\")\n",
    "    print(f\"  {title}\")\n",
    "    print(f\"{'─' * 50}\")\n",
    "\n",
    "def print_var_detail(var_name, cols_found, all_cols):\n",
    "    \"\"\"Affiche le détail d'une variable conceptuelle\"\"\"\n",
    "    if len(cols_found) == 0:\n",
    "        print(f\"  ❌ {var_name:<15} → NON TROUVÉE\")\n",
    "    elif len(cols_found) == 1 and cols_found[0] == var_name:\n",
    "        # Variable numérique (pas de dummies)\n",
    "        print(f\"  ✓ {var_name:<15} → numérique (1 colonne)\")\n",
    "    else:\n",
    "        # Variable catégorielle avec dummies\n",
    "        print(f\"  ✓ {var_name:<15} → catégorielle ({len(cols_found)} dummies)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Liste des variables conceptuelles retenues\n",
    "# ----------------------------\n",
    "FINAL_CONCEPTUAL_VARS = [\n",
    "    # 1) Âge\n",
    "    \"AGED\",\n",
    "    # 2) Statut d'activité / PCS / Public-privé\n",
    "    \"STATR\", \"PCSL\", \"GS\",\n",
    "    # 3) Mobilité domicile–travail\n",
    "    \"ILT\", \"TRANS\",\n",
    "    # 4) Taille/structure ménage\n",
    "    \"NPERR\", \"INFAM\",\n",
    "    # 5) Présence d'enfants\n",
    "    \"NE17FR\",\n",
    "    # 6) Typologie ménage\n",
    "    \"TDM8\", \"SFM\",\n",
    "    # 7) Logement\n",
    "    \"SURF\", \"HLML\", \"CATPC\",\n",
    "    # 8) Origine / migration\n",
    "    \"IMMI\", \"INATC\",\n",
    "    # 9) Formation / éducation\n",
    "    \"DIPL\", \"ETUD\",\n",
    "    # 10) Secteur d'activité\n",
    "    \"NA17\",\n",
    "    # 11) Santé\n",
    "    \"SANI\",\n",
    "]\n",
    "\n",
    "# Regroupement thématique pour l'affichage\n",
    "VAR_GROUPS = {\n",
    "    \"Âge\": [\"AGED\"],\n",
    "    \"Statut d'activité / PCS\": [\"STATR\", \"PCSL\", \"GS\"],\n",
    "    \"Mobilité domicile-travail\": [\"ILT\", \"TRANS\"],\n",
    "    \"Taille/structure ménage\": [\"NPERR\", \"INFAM\"],\n",
    "    \"Présence d'enfants\": [\"NE17FR\"],\n",
    "    \"Typologie ménage\": [\"TDM8\", \"SFM\"],\n",
    "    \"Logement\": [\"SURF\", \"HLML\", \"CATPC\"],\n",
    "    \"Origine / migration\": [\"IMMI\", \"INATC\"],\n",
    "    \"Formation / éducation\": [\"DIPL\", \"ETUD\"],\n",
    "    \"Secteur d'activité\": [\"NA17\"],\n",
    "    \"Santé\": [\"SANI\"],\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Démarrage et chargement\n",
    "# ----------------------------\n",
    "print_header(\"CODE 3 — FILTRAGE POST-ARBITRAGES ÉCONOMIQUES\")\n",
    "print(f\"\\n📅 Exécution : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print_section(\"1. CHARGEMENT DES DONNÉES\")\n",
    "\n",
    "if not os.path.exists(X_PATH):\n",
    "    raise FileNotFoundError(f\"X introuvable : {X_PATH}\")\n",
    "if not os.path.exists(Y_PATH):\n",
    "    raise FileNotFoundError(f\"y introuvable : {Y_PATH}\")\n",
    "\n",
    "print(f\"  📂 Répertoire source : {ML_READY_DIR}\")\n",
    "print(f\"  📄 Fichier X : X_ml_ready.csv\")\n",
    "print(f\"  📄 Fichier y : y.csv\")\n",
    "print(f\"\\n  ⏳ Chargement en cours...\")\n",
    "\n",
    "X = pd.read_csv(X_PATH, low_memory=False)\n",
    "y = pd.read_csv(Y_PATH, low_memory=False)\n",
    "\n",
    "print(f\"\\n  ✅ X chargé avec succès\")\n",
    "print(f\"     • Observations : {X.shape[0]:,}\")\n",
    "print(f\"     • Colonnes (features) : {X.shape[1]:,}\")\n",
    "print(f\"     • Taille mémoire : {X.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "print(f\"\\n  ✅ y chargé avec succès\")\n",
    "print(f\"     • Observations : {y.shape[0]:,}\")\n",
    "print(f\"     • Colonnes : {y.shape[1]}\")\n",
    "\n",
    "# Distribution de y\n",
    "if y.shape[1] == 1:\n",
    "    y_col = y.iloc[:, 0]\n",
    "    print(f\"\\n  📊 Distribution de la variable cible :\")\n",
    "    value_counts = y_col.value_counts().sort_index()\n",
    "    total = len(y_col)\n",
    "    for val, count in value_counts.items():\n",
    "        pct = count / total * 100\n",
    "        print(f\"     • {val} : {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Sélection des colonnes\n",
    "# ----------------------------\n",
    "print_section(\"2. MAPPING VARIABLES CONCEPTUELLES → COLONNES\")\n",
    "\n",
    "print(f\"\\n  Variables conceptuelles demandées : {len(FINAL_CONCEPTUAL_VARS)}\")\n",
    "print(f\"  Colonnes disponibles dans X : {X.shape[1]:,}\")\n",
    "\n",
    "cols = list(X.columns)\n",
    "keep_cols = []\n",
    "not_found = []\n",
    "var_mapping = {}  # Pour stocker le détail du mapping\n",
    "\n",
    "for v in FINAL_CONCEPTUAL_VARS:\n",
    "    matched = []\n",
    "    \n",
    "    # Cas numérique ou déjà encodé en une seule colonne\n",
    "    if v in cols:\n",
    "        matched.append(v)\n",
    "    \n",
    "    # Cas catégoriel one-hot\n",
    "    prefix = v + \"_\"\n",
    "    matched += [c for c in cols if c.startswith(prefix)]\n",
    "    \n",
    "    # Dédoublonnage\n",
    "    matched = sorted(set(matched))\n",
    "    var_mapping[v] = matched\n",
    "    \n",
    "    if len(matched) == 0:\n",
    "        not_found.append(v)\n",
    "    else:\n",
    "        keep_cols.extend(matched)\n",
    "\n",
    "# Unique + ordre stable\n",
    "keep_cols = list(dict.fromkeys(keep_cols))\n",
    "\n",
    "if len(keep_cols) == 0:\n",
    "    raise ValueError(\"Aucune colonne matchée. Vérifie les noms et le format des dummies.\")\n",
    "\n",
    "# Affichage détaillé par groupe thématique\n",
    "print_section(\"3. DÉTAIL PAR GROUPE THÉMATIQUE\")\n",
    "\n",
    "total_numeric = 0\n",
    "total_categorical = 0\n",
    "total_dummies = 0\n",
    "\n",
    "for group_name, group_vars in VAR_GROUPS.items():\n",
    "    print(f\"\\n  📁 {group_name}\")\n",
    "    group_cols = 0\n",
    "    \n",
    "    for v in group_vars:\n",
    "        matched = var_mapping.get(v, [])\n",
    "        if len(matched) == 0:\n",
    "            print(f\"     ❌ {v:<12} → NON TROUVÉE\")\n",
    "        elif len(matched) == 1 and matched[0] == v:\n",
    "            print(f\"     ✓ {v:<12} → numérique (1 col)\")\n",
    "            total_numeric += 1\n",
    "            group_cols += 1\n",
    "        else:\n",
    "            print(f\"     ✓ {v:<12} → catégorielle ({len(matched)} dummies)\")\n",
    "            total_categorical += 1\n",
    "            total_dummies += len(matched)\n",
    "            group_cols += len(matched)\n",
    "    \n",
    "    print(f\"     {'─' * 30}\")\n",
    "    print(f\"     Sous-total groupe : {group_cols} colonnes\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Statistiques globales\n",
    "# ----------------------------\n",
    "print_section(\"4. STATISTIQUES GLOBALES DE SÉLECTION\")\n",
    "\n",
    "print(f\"\\n  📊 Résumé du mapping :\")\n",
    "print(f\"     • Variables conceptuelles trouvées : {len(FINAL_CONCEPTUAL_VARS) - len(not_found)}/{len(FINAL_CONCEPTUAL_VARS)}\")\n",
    "print(f\"     • Variables conceptuelles manquantes : {len(not_found)}\")\n",
    "if not_found:\n",
    "    print(f\"       → {', '.join(not_found)}\")\n",
    "\n",
    "print(f\"\\n  📐 Dimensions :\")\n",
    "print(f\"     • Variables numériques : {total_numeric}\")\n",
    "print(f\"     • Variables catégorielles : {total_categorical}\")\n",
    "print(f\"     • Total dummies catégorielles : {total_dummies}\")\n",
    "print(f\"     • Total colonnes sélectionnées : {len(keep_cols)}\")\n",
    "\n",
    "print(f\"\\n  📉 Réduction de dimension :\")\n",
    "print(f\"     • Colonnes initiales : {X.shape[1]:,}\")\n",
    "print(f\"     • Colonnes conservées : {len(keep_cols)}\")\n",
    "print(f\"     • Colonnes supprimées : {X.shape[1] - len(keep_cols):,}\")\n",
    "print(f\"     • Taux de réduction : {(1 - len(keep_cols)/X.shape[1])*100:.1f}%\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Construction de X_final\n",
    "# ----------------------------\n",
    "print_section(\"5. CONSTRUCTION DE LA BASE FINALE\")\n",
    "\n",
    "X_final = X[keep_cols].copy()\n",
    "\n",
    "print(f\"\\n  📦 X_final créé :\")\n",
    "print(f\"     • Shape : {X_final.shape[0]:,} × {X_final.shape[1]}\")\n",
    "print(f\"     • Taille mémoire : {X_final.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Vérification des valeurs manquantes\n",
    "missing_counts = X_final.isnull().sum()\n",
    "cols_with_missing = missing_counts[missing_counts > 0]\n",
    "\n",
    "print(f\"\\n  🔍 Vérification des valeurs manquantes :\")\n",
    "if len(cols_with_missing) == 0:\n",
    "    print(f\"     ✅ Aucune valeur manquante détectée\")\n",
    "else:\n",
    "    print(f\"     ⚠️  {len(cols_with_missing)} colonnes avec valeurs manquantes :\")\n",
    "    for col, count in cols_with_missing.head(10).items():\n",
    "        pct = count / len(X_final) * 100\n",
    "        print(f\"        • {col}: {count:,} ({pct:.2f}%)\")\n",
    "    if len(cols_with_missing) > 10:\n",
    "        print(f\"        ... et {len(cols_with_missing) - 10} autres colonnes\")\n",
    "\n",
    "# Aperçu des types de données\n",
    "print(f\"\\n  📋 Types de données :\")\n",
    "dtype_counts = X_final.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"     • {dtype}: {count} colonnes\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Exports\n",
    "# ----------------------------\n",
    "print_section(\"6. EXPORT DES FICHIERS\")\n",
    "\n",
    "print(f\"\\n  ⏳ Export en cours...\")\n",
    "\n",
    "X_final.to_csv(OUT_X, index=False)\n",
    "y.to_csv(OUT_Y, index=False)\n",
    "\n",
    "# Calcul des tailles de fichiers\n",
    "x_size = os.path.getsize(OUT_X) / 1024**2\n",
    "y_size = os.path.getsize(OUT_Y) / 1024**2\n",
    "\n",
    "print(f\"\\n  ✅ Fichiers exportés avec succès :\")\n",
    "print(f\"     📄 X_final : {OUT_X}\")\n",
    "print(f\"        → Taille : {x_size:.1f} MB\")\n",
    "print(f\"     📄 y_final : {OUT_Y}\")\n",
    "print(f\"        → Taille : {y_size:.1f} MB\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Génération du rapport\n",
    "# ----------------------------\n",
    "lines = []\n",
    "lines.append(\"=\" * 70)\n",
    "lines.append(\"RAPPORT DE FILTRAGE POST-ARBITRAGES\")\n",
    "lines.append(f\"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "lines.append(\"=\" * 70)\n",
    "lines.append(\"\")\n",
    "lines.append(\"SOURCES\")\n",
    "lines.append(\"-\" * 40)\n",
    "lines.append(f\"X source : {X_PATH}\")\n",
    "lines.append(f\"y source : {Y_PATH}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"STATISTIQUES\")\n",
    "lines.append(\"-\" * 40)\n",
    "lines.append(f\"Variables conceptuelles demandées : {len(FINAL_CONCEPTUAL_VARS)}\")\n",
    "lines.append(f\"Variables conceptuelles trouvées : {len(FINAL_CONCEPTUAL_VARS) - len(not_found)}\")\n",
    "lines.append(f\"Variables conceptuelles manquantes : {len(not_found)}\")\n",
    "if not_found:\n",
    "    lines.append(f\"  → {', '.join(not_found)}\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"Colonnes initiales : {X.shape[1]}\")\n",
    "lines.append(f\"Colonnes conservées : {len(keep_cols)}\")\n",
    "lines.append(f\"Taux de réduction : {(1 - len(keep_cols)/X.shape[1])*100:.1f}%\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"Shape X_final : {X_final.shape[0]} × {X_final.shape[1]}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"DÉTAIL PAR GROUPE THÉMATIQUE\")\n",
    "lines.append(\"-\" * 40)\n",
    "\n",
    "for group_name, group_vars in VAR_GROUPS.items():\n",
    "    lines.append(f\"\\n[{group_name}]\")\n",
    "    for v in group_vars:\n",
    "        matched = var_mapping.get(v, [])\n",
    "        if len(matched) == 0:\n",
    "            lines.append(f\"  ❌ {v} : NON TROUVÉE\")\n",
    "        elif len(matched) == 1 and matched[0] == v:\n",
    "            lines.append(f\"  ✓ {v} : numérique (1 colonne)\")\n",
    "        else:\n",
    "            lines.append(f\"  ✓ {v} : catégorielle ({len(matched)} dummies)\")\n",
    "            for col in matched[:5]:\n",
    "                lines.append(f\"      - {col}\")\n",
    "            if len(matched) > 5:\n",
    "                lines.append(f\"      ... et {len(matched) - 5} autres\")\n",
    "\n",
    "lines.append(\"\")\n",
    "lines.append(\"LISTE COMPLÈTE DES COLONNES CONSERVÉES\")\n",
    "lines.append(\"-\" * 40)\n",
    "for i, col in enumerate(keep_cols, 1):\n",
    "    lines.append(f\"  {i:3}. {col}\")\n",
    "\n",
    "lines.append(\"\")\n",
    "lines.append(\"FICHIERS EXPORTÉS\")\n",
    "lines.append(\"-\" * 40)\n",
    "lines.append(f\"X_final : {OUT_X}\")\n",
    "lines.append(f\"y_final : {OUT_Y}\")\n",
    "\n",
    "with open(OUT_REPORT, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(f\"     📄 Rapport : {OUT_REPORT}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Résumé final\n",
    "# ----------------------------\n",
    "print_header(\"RÉSUMÉ FINAL\")\n",
    "\n",
    "print(f\"\"\"\n",
    "  ┌─────────────────────────────────────────────────────────┐\n",
    "  │  FILTRAGE POST-ARBITRAGES TERMINÉ AVEC SUCCÈS           │\n",
    "  ├─────────────────────────────────────────────────────────┤\n",
    "  │  Observations     : {X_final.shape[0]:>10,}                       │\n",
    "  │  Features initiales: {X.shape[1]:>10,}                       │\n",
    "  │  Features finales  : {X_final.shape[1]:>10}                       │\n",
    "  │  Réduction         : {(1 - len(keep_cols)/X.shape[1])*100:>10.1f}%                      │\n",
    "  ├─────────────────────────────────────────────────────────┤\n",
    "  │  Variables manquantes : {len(not_found):<5}                            │\n",
    "  └─────────────────────────────────────────────────────────┘\n",
    "\"\"\")\n",
    "\n",
    "# Liste des 10 premières colonnes conservées\n",
    "print(\"  📋 Aperçu des colonnes conservées (10 premières) :\")\n",
    "for i, col in enumerate(keep_cols[:10], 1):\n",
    "    print(f\"     {i:2}. {col}\")\n",
    "if len(keep_cols) > 10:\n",
    "    print(f\"     ... et {len(keep_cols) - 10} autres colonnes\")\n",
    "\n",
    "print(f\"\\n  📁 Fichiers disponibles dans :\")\n",
    "print(f\"     {OUT_DIR}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b137dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONSTRUCTION DE LA BDD ML FINALE (avant Elastic Net)\n",
    "# Basée exclusivement sur les arbitrages économiques validés\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Paramètres\n",
    "# ----------------------------\n",
    "IN_PATH = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/BDD_FINAL_new.csv\"\n",
    "SEP = \";\"\n",
    "\n",
    "OUT_DIR = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/ML_FINAL\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "TARGET = \"TRANSFRONTALIER\"\n",
    "WEIGHT = \"IPONDI\"\n",
    "\n",
    "# ----------------------------\n",
    "# Variables retenues\n",
    "# ----------------------------\n",
    "\n",
    "NUM_VARS = [\n",
    "    \"AGED\", \"NPERR\", \"ILT\", \"DIPL\", \"GS\", \"EMPL\"\n",
    "]\n",
    "\n",
    "CAT_VARS = [\n",
    "    \"STATR\", \"STAT_GSEC\", \"TRANS\", \"TYPL\", \"TYPMR\",\n",
    "    \"SFM\", \"TDM8\", \"HLML\", \"CATPC\", \"INFAM\",\n",
    "    \"NE17FR\", \"IMMI\", \"INATC\", \"ETUD\",\n",
    "    \"NA17\", \"SANI\", \"SEXE\"\n",
    "]\n",
    "\n",
    "FINAL_VARS = NUM_VARS + CAT_VARS\n",
    "\n",
    "# ----------------------------\n",
    "# Chargement (colonnes utiles seulement → rapide)\n",
    "# ----------------------------\n",
    "USECOLS = FINAL_VARS + [TARGET, WEIGHT]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    IN_PATH,\n",
    "    sep=SEP,\n",
    "    usecols=USECOLS,\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "print(f\"[OK] Lecture : {df.shape[0]:,} lignes | {df.shape[1]} colonnes\")\n",
    "\n",
    "# ----------------------------\n",
    "# Séparation X / y / w\n",
    "# ----------------------------\n",
    "y = df[TARGET].astype(int)\n",
    "w = df[WEIGHT]\n",
    "\n",
    "X = df[FINAL_VARS].copy()\n",
    "\n",
    "# ----------------------------\n",
    "# Imputation simple et propre\n",
    "# ----------------------------\n",
    "\n",
    "# Numériques → médiane\n",
    "for col in NUM_VARS:\n",
    "    med = X[col].median()\n",
    "    X[col] = pd.to_numeric(X[col], errors=\"coerce\").fillna(med)\n",
    "\n",
    "# Catégorielles → \"__MISSING__\"\n",
    "for col in CAT_VARS:\n",
    "    X[col] = (\n",
    "        X[col]\n",
    "        .astype(\"object\")\n",
    "        .where(~X[col].isna(), \"__MISSING__\")\n",
    "        .astype(str)\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# Vérifications finales\n",
    "# ----------------------------\n",
    "assert X.isna().sum().sum() == 0, \"NaN restants dans X\"\n",
    "assert set(y.unique()).issubset({0, 1}), \"Cible non binaire\"\n",
    "\n",
    "print(\"[OK] Imputation terminée\")\n",
    "print(\"[OK] Variables finales :\", X.shape[1])\n",
    "\n",
    "# ----------------------------\n",
    "# Exports\n",
    "# ----------------------------\n",
    "X.to_csv(os.path.join(OUT_DIR, \"X_ml_final.csv\"), index=False)\n",
    "y.to_csv(os.path.join(OUT_DIR, \"y.csv\"), index=False)\n",
    "w.to_csv(os.path.join(OUT_DIR, \"weights.csv\"), index=False)\n",
    "\n",
    "print(\"\\n[OK] Exports terminés :\")\n",
    "print(\" - X_ml_final.csv\")\n",
    "print(\" - y.csv\")\n",
    "print(\" - weights.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9328c9",
   "metadata": {},
   "source": [
    "# Identitifcation et reconstruction des couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eb17cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "CODE 1 — MATCHING COUPLES (LPRM=1/2) — COMMUNE/CANTVILLE + NUMMI + NUMF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "\n",
      "[1] Chargement CSV …\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 56\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# 1) CHARGEMENT\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[1;32m     55\u001b[0m tprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[1] Chargement CSV …\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(INPUT_PATH, sep\u001b[38;5;241m=\u001b[39mSEP, encoding\u001b[38;5;241m=\u001b[39mENC, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     57\u001b[0m tprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOK: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m lignes | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m colonnes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m tprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColonnes clés attendues : COMMUNE, CANTVILLE, NIVEAU_APPARIEMENT, NUMMI, NUMF, LPRM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m         nrows\n\u001b[1;32m   1925\u001b[0m     )\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread(nrows)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CODE 1 — Reconstruction des couples (matching STRICT LPRM=1/2)\n",
    "Stratégie validée (Picard) :\n",
    "- ID_ZONE = COMMUNE si NIVEAU_APPARIEMENT='1_DEPCOM' sinon CANTVILLE\n",
    "- ID_FAMILLE = ID_ZONE + NUMMI + NUMF  (SANS restriction sur NUMF)\n",
    "- Candidats = individus avec LPRM ∈ {1,2} et NUMMI != 'Z'\n",
    "- Couples sûrs = ID_FAMILLE avec exactement 1 LPRM=1 et 1 LPRM=2\n",
    "Sorties :\n",
    "- CSV \"long\" : 2 lignes par couple, avec ID_COUPLE = ID_FAMILLE\n",
    "- Excel sample 10k lignes pour contrôle visuel\n",
    "- Diagnostics console : tailles, patterns LPRM, taux de reconstruction, audits NA/collisions\n",
    "\n",
    "Auteur : Projet INSEE — Mobilité Transfrontalière\n",
    "Date : Janvier 2026\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# ============================================================\n",
    "# PARAMÈTRES (adapte tes chemins)\n",
    "# ============================================================\n",
    "INPUT_PATH = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/BDD_FINAL_CLEAN.csv\"\n",
    "\n",
    "OUT_COUPLES_LONG_CSV = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/COUPLES_LONG_MATCHED.csv\"\n",
    "OUT_SAMPLE_XLSX      = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/COUPLES_LONG_sample10k.xlsx\"\n",
    "\n",
    "SEP = \";\"  # ton fichier est en ;\n",
    "ENC = \"utf-8\"\n",
    "\n",
    "SAMPLE_N = 10_000\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ============================================================\n",
    "# HELPERS\n",
    "# ============================================================\n",
    "def pct(x, n):\n",
    "    return 100 * x / n if n else 0.0\n",
    "\n",
    "def tprint(msg):\n",
    "    print(msg, flush=True)\n",
    "\n",
    "# ============================================================\n",
    "# 0) START\n",
    "# ============================================================\n",
    "t0 = time.time()\n",
    "tprint(\"=\" * 85)\n",
    "tprint(\"CODE 1 — MATCHING COUPLES (LPRM=1/2) — COMMUNE/CANTVILLE + NUMMI + NUMF\")\n",
    "tprint(\"=\" * 85)\n",
    "\n",
    "# ============================================================\n",
    "# 1) CHARGEMENT\n",
    "# ============================================================\n",
    "tprint(\"\\n[1] Chargement CSV …\")\n",
    "df = pd.read_csv(INPUT_PATH, sep=SEP, encoding=ENC, low_memory=False)\n",
    "tprint(f\"OK: {df.shape[0]:,} lignes | {df.shape[1]} colonnes\")\n",
    "tprint(\"Colonnes clés attendues : COMMUNE, CANTVILLE, NIVEAU_APPARIEMENT, NUMMI, NUMF, LPRM\")\n",
    "\n",
    "required = [\"COMMUNE\", \"CANTVILLE\", \"NIVEAU_APPARIEMENT\", \"NUMMI\", \"NUMF\", \"LPRM\"]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Colonnes manquantes: {missing}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) NORMALISATION DES TYPES (CRITIQUE)\n",
    "# ============================================================\n",
    "tprint(\"\\n[2] Normalisation types …\")\n",
    "\n",
    "# NUMMI / NUMF sont object avec valeurs 'Z' possibles (cf dictionnaire)\n",
    "df[\"NUMMI\"] = df[\"NUMMI\"].astype(\"string\").str.strip().str.upper()\n",
    "df[\"NUMF\"]  = df[\"NUMF\"].astype(\"string\").str.strip().str.upper()\n",
    "\n",
    "# LPRM est float avec manquants ~0.63% : on force Int64 propre\n",
    "df[\"LPRM\"] = pd.to_numeric(df[\"LPRM\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# COMMUNE / CANTVILLE int64 -> on garde, mais on sécurise NA (normalement 0% chez toi)\n",
    "# On convertit en Int64 pour pouvoir détecter NA si jamais un jour ça arrive.\n",
    "df[\"COMMUNE\"]   = pd.to_numeric(df[\"COMMUNE\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df[\"CANTVILLE\"] = pd.to_numeric(df[\"CANTVILLE\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Niveau appariement\n",
    "df[\"NIVEAU_APPARIEMENT\"] = df[\"NIVEAU_APPARIEMENT\"].astype(\"string\").str.strip()\n",
    "\n",
    "# ============================================================\n",
    "# 3) AUDIT NA / CAS IMPOSSIBLES\n",
    "# ============================================================\n",
    "tprint(\"\\n[3] Audit NA / cohérence minimale …\")\n",
    "\n",
    "na_commune = df[\"COMMUNE\"].isna().sum()\n",
    "na_cant    = df[\"CANTVILLE\"].isna().sum()\n",
    "na_nummi   = df[\"NUMMI\"].isna().sum()\n",
    "na_numf    = df[\"NUMF\"].isna().sum()\n",
    "na_lprm    = df[\"LPRM\"].isna().sum()\n",
    "\n",
    "tprint(f\"NA COMMUNE   : {na_commune:,}\")\n",
    "tprint(f\"NA CANTVILLE : {na_cant:,}\")\n",
    "tprint(f\"NA NUMMI     : {na_nummi:,}\")\n",
    "tprint(f\"NA NUMF      : {na_numf:,}\")\n",
    "tprint(f\"NA LPRM      : {na_lprm:,}\")\n",
    "\n",
    "# Cas incohérents (NIVEAU_APPARIEMENT demande COMMUNE mais COMMUNE manquante, etc.)\n",
    "mask_need_commune = (df[\"NIVEAU_APPARIEMENT\"] == \"1_DEPCOM\") & (df[\"COMMUNE\"].isna())\n",
    "mask_need_cant    = (df[\"NIVEAU_APPARIEMENT\"] == \"2_CANTVILLE\") & (df[\"CANTVILLE\"].isna())\n",
    "\n",
    "tprint(f\"Incohérences: 1_DEPCOM mais COMMUNE NA     : {mask_need_commune.sum():,}\")\n",
    "tprint(f\"Incohérences: 2_CANTVILLE mais CANTVILLE NA: {mask_need_cant.sum():,}\")\n",
    "\n",
    "# ============================================================\n",
    "# 4) CONSTRUCTION ID_ZONE puis ID_FAMILLE\n",
    "# ============================================================\n",
    "tprint(\"\\n[4] Construction ID_ZONE + ID_FAMILLE …\")\n",
    "\n",
    "# ID_ZONE dépend du niveau d'appariement\n",
    "id_zone = np.where(\n",
    "    df[\"NIVEAU_APPARIEMENT\"].to_numpy() == \"1_DEPCOM\",\n",
    "    \"COM_\" + df[\"COMMUNE\"].astype(\"string\").to_numpy(),\n",
    "    \"CAN_\" + df[\"CANTVILLE\"].astype(\"string\").to_numpy()\n",
    ")\n",
    "\n",
    "# ID_FAMILLE = zone + NUMMI + NUMF (sans restriction sur NUMF)\n",
    "df[\"ID_FAMILLE\"] = (\n",
    "    pd.Series(id_zone, index=df.index).astype(\"string\")\n",
    "    + \"_M\" + df[\"NUMMI\"]\n",
    "    + \"_F\" + df[\"NUMF\"]\n",
    ")\n",
    "\n",
    "tprint(\"OK: ID_FAMILLE créée\")\n",
    "tprint(f\"Nb ID_FAMILLE distincts (toute base): {df['ID_FAMILLE'].nunique():,}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5) CANDIDATS COUPLE (LPRM in {1,2} & NUMMI != 'Z')\n",
    "# ============================================================\n",
    "tprint(\"\\n[5] Sélection candidats (LPRM ∈ {1,2} & NUMMI != 'Z') …\")\n",
    "\n",
    "cand = df[(df[\"LPRM\"].isin([1, 2])) & (df[\"NUMMI\"] != \"Z\")].copy()\n",
    "\n",
    "tprint(f\"Candidats: {len(cand):,} individus\")\n",
    "tprint(f\"  dont LPRM=1 : {(cand['LPRM'] == 1).sum():,}\")\n",
    "tprint(f\"  dont LPRM=2 : {(cand['LPRM'] == 2).sum():,}\")\n",
    "\n",
    "# ============================================================\n",
    "# 6) DIAGNOSTICS STRUCTURE AVANT MATCHING\n",
    "# ============================================================\n",
    "tprint(\"\\n[6] Diagnostics avant matching …\")\n",
    "\n",
    "# Taille du groupe (sur candidats) par ID_FAMILLE\n",
    "sizes = cand.groupby(\"ID_FAMILLE\").size()\n",
    "tprint(\"Taille des groupes (candidats) — top fréquences :\")\n",
    "print(sizes.value_counts().sort_index().head(10).to_string())\n",
    "\n",
    "# Pattern LPRM (comptes de 1 et 2 par famille) — vectorisé\n",
    "tab = pd.crosstab(cand[\"ID_FAMILLE\"], cand[\"LPRM\"])\n",
    "if 1 not in tab.columns: tab[1] = 0\n",
    "if 2 not in tab.columns: tab[2] = 0\n",
    "\n",
    "# Quelques patterns clés\n",
    "n_ids = tab.shape[0]\n",
    "pattern_11 = ((tab[1] == 1) & (tab[2] == 1)).sum()\n",
    "pattern_10 = ((tab[1] == 1) & (tab[2] == 0)).sum()\n",
    "pattern_01 = ((tab[1] == 0) & (tab[2] == 1)).sum()\n",
    "pattern_12p = ((tab[1] == 1) & (tab[2] >= 2)).sum()\n",
    "pattern_21p = ((tab[1] >= 2) & (tab[2] == 1)).sum()\n",
    "pattern_22p = ((tab[1] >= 2) & (tab[2] >= 2)).sum()\n",
    "\n",
    "tprint(\"\\nPatterns LPRM par ID_FAMILLE (sur candidats) :\")\n",
    "tprint(f\"  (1,1) exact  -> couples sûrs potentiels : {pattern_11:,} ({pct(pattern_11, n_ids):.2f}%)\")\n",
    "tprint(f\"  (1,0)        -> ref sans conjoint      : {pattern_10:,} ({pct(pattern_10, n_ids):.2f}%)\")\n",
    "tprint(f\"  (0,1)        -> conjoint sans ref      : {pattern_01:,} ({pct(pattern_01, n_ids):.2f}%)\")\n",
    "tprint(f\"  (1,>=2)      -> 1 ref / multi conj     : {pattern_12p:,} ({pct(pattern_12p, n_ids):.2f}%)\")\n",
    "tprint(f\"  (>=2,1)      -> multi ref / 1 conj     : {pattern_21p:,} ({pct(pattern_21p, n_ids):.2f}%)\")\n",
    "tprint(f\"  (>=2,>=2)    -> multi/multi            : {pattern_22p:,} ({pct(pattern_22p, n_ids):.2f}%)\")\n",
    "\n",
    "# ============================================================\n",
    "# 7) MATCHING STRICT : exactement 1 LPRM=1 et 1 LPRM=2\n",
    "# ============================================================\n",
    "tprint(\"\\n[7] Matching strict (1 ref + 1 conjoint) …\")\n",
    "\n",
    "ok_ids = tab.index[(tab[1] == 1) & (tab[2] == 1)]\n",
    "tprint(f\"ID_FAMILLE matchés (couples sûrs) : {len(ok_ids):,}\")\n",
    "\n",
    "couples_long = cand[cand[\"ID_FAMILLE\"].isin(ok_ids)].copy()\n",
    "\n",
    "# Sécurité : il doit y avoir exactement 2 lignes par ID_FAMILLE\n",
    "check_sizes = couples_long.groupby(\"ID_FAMILLE\").size()\n",
    "bad = (check_sizes != 2).sum()\n",
    "if bad:\n",
    "    tprint(f\"⚠️ ATTENTION: {bad:,} familles n'ont pas exactement 2 lignes après filtrage (devrait être 0)\")\n",
    "else:\n",
    "    tprint(\"OK: 2 lignes par couple (contrôle passé)\")\n",
    "\n",
    "# ID_COUPLE = ID_FAMILLE\n",
    "couples_long[\"ID_COUPLE\"] = couples_long[\"ID_FAMILLE\"]\n",
    "\n",
    "# ROLE_COUPLE pour lisibilité\n",
    "couples_long[\"ROLE_COUPLE\"] = np.where(couples_long[\"LPRM\"] == 1, \"REF\", \"CONJ\")\n",
    "\n",
    "tprint(f\"Couples (long) : {len(couples_long):,} lignes -> {couples_long['ID_COUPLE'].nunique():,} couples\")\n",
    "\n",
    "# ============================================================\n",
    "# 8) PETITS CHECKS QUALITATIFS RAPIDES (sans score, ça c'est Code 2)\n",
    "# ============================================================\n",
    "tprint(\"\\n[8] Checks rapides …\")\n",
    "\n",
    "# Si COUPLE existe, on regarde la cohérence déclarative sans filtrer ici\n",
    "if \"COUPLE\" in couples_long.columns:\n",
    "    tmp = pd.crosstab(couples_long[\"ROLE_COUPLE\"], couples_long[\"COUPLE\"], margins=True)\n",
    "    tprint(\"Table ROLE_COUPLE × COUPLE (sur couples matchés) :\")\n",
    "    print(tmp.to_string())\n",
    "\n",
    "# Distribution NUMF dans les couples (pour vérifier que tu n'as pas “involontairement” restreint)\n",
    "tprint(\"\\nDistribution NUMF (sur couples matchés) :\")\n",
    "print(couples_long[\"NUMF\"].value_counts(dropna=False).head(10).to_string())\n",
    "\n",
    "# ============================================================\n",
    "# 9) EXPORT CSV COUPLES LONG\n",
    "# ============================================================\n",
    "tprint(\"\\n[9] Export CSV couples (long) …\")\n",
    "# On met devant les colonnes utiles\n",
    "front_cols = [\"ID_COUPLE\", \"ID_FAMILLE\", \"ROLE_COUPLE\", \"LPRM\",\n",
    "              \"NIVEAU_APPARIEMENT\", \"COMMUNE\", \"CANTVILLE\", \"NUMMI\", \"NUMF\"]\n",
    "front_cols = [c for c in front_cols if c in couples_long.columns]\n",
    "cols = front_cols + [c for c in couples_long.columns if c not in front_cols]\n",
    "\n",
    "couples_long[cols].to_csv(OUT_COUPLES_LONG_CSV, index=False, sep=SEP, encoding=ENC)\n",
    "tprint(f\"OK: couples exportés -> {OUT_COUPLES_LONG_CSV}\")\n",
    "tprint(f\"    {couples_long['ID_COUPLE'].nunique():,} couples | {len(couples_long):,} lignes\")\n",
    "\n",
    "# ============================================================\n",
    "# 10) SAMPLE EXCEL 10k (contrôle visuel)\n",
    "# ============================================================\n",
    "tprint(\"\\n[10] Export Excel sample 10k lignes …\")\n",
    "\n",
    "sample = couples_long.sample(n=min(SAMPLE_N, len(couples_long)), random_state=RANDOM_STATE).copy()\n",
    "front2 = [\"ID_COUPLE\", \"ROLE_COUPLE\", \"LPRM\", \"SEXE\", \"AGEREV\", \"DIPL\", \"ILT\", \"COUPLE\",\n",
    "          \"TYPMR\", \"TDM8\", \"MOCO_DET\", \"STAT_CONJ\",\n",
    "          \"NIVEAU_APPARIEMENT\", \"COMMUNE\", \"CANTVILLE\", \"NUMMI\", \"NUMF\", \"DEPT\", \"DEPCOM\"]\n",
    "front2 = [c for c in front2 if c in sample.columns]\n",
    "sample = sample[front2 + [c for c in sample.columns if c not in front2]]\n",
    "\n",
    "sample.to_excel(OUT_SAMPLE_XLSX, index=False)\n",
    "tprint(f\"OK: sample Excel -> {OUT_SAMPLE_XLSX}\")\n",
    "\n",
    "# ============================================================\n",
    "# 11) FIN\n",
    "# ============================================================\n",
    "dt = time.time() - t0\n",
    "tprint(\"\\n\" + \"=\" * 85)\n",
    "tprint(f\"TERMINÉ — temps total: {dt:.1f}s\")\n",
    "tprint(\"=\" * 85)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90630a4",
   "metadata": {},
   "source": [
    "# Validation du matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8118871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "CODE 2 — VALIDATION / COHÉRENCE DES COUPLES (LONG -> LONG enrichi + WIDE)\n",
      "====================================================================================================\n",
      "\n",
      "[1] Chargement …\n",
      "OK: 253,948 lignes | 109 colonnes\n",
      "Colonnes présentes (extrait): ['ID_COUPLE', 'ID_FAMILLE', 'ROLE_COUPLE', 'LPRM', 'NIVEAU_APPARIEMENT', 'COMMUNE', 'CANTVILLE', 'NUMMI', 'NUMF', 'ARM', 'DCFLT', 'DCLT', 'AGEREVQ', 'GS', 'DEROU']\n",
      "\n",
      "[2] Normalisation types …\n",
      "OK: types normalisés.\n",
      "\n",
      "[3] Checks structurels …\n",
      "Couples avec taille != 2: 0 (attendu 0)\n",
      "Couples sans exactement 1 REF et 1 CONJ: 0 (attendu 0)\n",
      "Nb couples: 126,974\n",
      "\n",
      "[4] Construction table couple-level (REF/CONJ côte à côte) …\n",
      "OK: couple-level -> 126,974 lignes | 19 colonnes\n",
      "\n",
      "[5] Diagnostics bruts …\n",
      "COUPLE déclaratif parfait (1 & 1): 99.55%\n",
      "Couples avec au moins 1 membre COUPLE!=1: 574 (0.45%)\n",
      "\n",
      "Top modalités NUMF_REF:\n",
      "NUMF_REF\n",
      "1    126965\n",
      "Z         9\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Distribution STAT_CONJ (REF) :\n",
      "STAT_CONJ_REF\n",
      "1    75123\n",
      "2    21981\n",
      "3    28703\n",
      "4       35\n",
      "5      305\n",
      "6      827\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribution STAT_CONJ (CONJ) :\n",
      "STAT_CONJ_CONJ\n",
      "1    75171\n",
      "2    21967\n",
      "3    28728\n",
      "4       24\n",
      "5      244\n",
      "6      840\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[6] Calcul SCORE_QUALITE (0–5) …\n",
      "Distribution SCORE_QUALITE :\n",
      "SCORE_QUALITE\n",
      "2         1\n",
      "3        38\n",
      "4      2293\n",
      "5    124642\n",
      "Name: count, dtype: int64\n",
      "Score moyen: 4.98 / 5\n",
      "\n",
      "[7] Variables analytiques (Y_REF/Y_CONJ + écart âge) …\n",
      "\n",
      "Table décision transfrontalière (ILT==7):\n",
      "Y_CONJ       0      1     All\n",
      "Y_REF                        \n",
      "0       108856   5032  113888\n",
      "1         7006   6080   13086\n",
      "All     115862  11112  126974\n",
      "\n",
      "[8] Merge vers LONG + exports …\n",
      "OK: LONG enrichi -> /Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/COUPLES_LONG_VALIDATED.csv\n",
      "OK: WIDE validé -> /Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/COUPLES_WIDE_VALIDATED.csv\n",
      "OK: Sample Excel -> /Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/COUPLES_VALIDATED_sample10k.xlsx\n",
      "\n",
      "====================================================================================================\n",
      "TERMINÉ — temps total: 47.1s\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# ============================================================\n",
    "# PARAMÈTRES\n",
    "# ============================================================\n",
    "INPUT_COUPLES_LONG = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/COUPLES_LONG_MATCHED.csv\"\n",
    "\n",
    "OUT_LONG_ENRICHED = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/COUPLES_LONG_VALIDATED.csv\"\n",
    "OUT_WIDE = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/COUPLES_WIDE_VALIDATED.csv\"\n",
    "OUT_SAMPLE_XLSX = \"/Users/mehdifehri/Desktop/Projet INSEE/BDD/BDD GE/BDD Clean/COUPLES_VALIDATED_sample10k.xlsx\"\n",
    "\n",
    "SAMPLE_N = 10_000\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ---- PARAMS SCORE (à ajuster seulement si tu veux) ----\n",
    "TYPMR_COUPLE_SET = {41, 42, 43, 44}          # selon ton plan (classique)\n",
    "TDM8_COUPLE_SET = {200, 302, 310, 311}       # selon ton plan (classique)\n",
    "MOCO_ADULTE_SET = {\"A0\", \"A1\", \"A2\", \"A3\"}    # selon ton plan (adulte famille)\n",
    "\n",
    "# STAT_CONJ : INCONNU -> on met par défaut {1,2,3} MAIS on imprimera la distribution.\n",
    "STAT_CONJ_UNION_SET = {1, 2, 3}\n",
    "\n",
    "# ============================================================\n",
    "# 1) CHARGEMENT\n",
    "# ============================================================\n",
    "print(\"=\" * 100)\n",
    "print(\"CODE 2 — VALIDATION / COHÉRENCE DES COUPLES (LONG -> LONG enrichi + WIDE)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\n[1] Chargement …\")\n",
    "df = pd.read_csv(INPUT_COUPLES_LONG, sep=\";\", encoding=\"utf-8\", low_memory=False)\n",
    "print(f\"OK: {df.shape[0]:,} lignes | {df.shape[1]} colonnes\")\n",
    "print(\"Colonnes présentes (extrait):\", list(df.columns[:15]))\n",
    "\n",
    "required = [\"ID_COUPLE\", \"ROLE_COUPLE\", \"LPRM\"]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Colonnes manquantes: {missing}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) NORMALISATION TYPES\n",
    "# ============================================================\n",
    "print(\"\\n[2] Normalisation types …\")\n",
    "\n",
    "df[\"ROLE_COUPLE\"] = df[\"ROLE_COUPLE\"].astype(\"string\").str.strip().str.upper()\n",
    "\n",
    "# Numériques (robustes)\n",
    "for c in [\"COUPLE\", \"TYPMR\", \"STAT_CONJ\", \"AGEREV\", \"AGED\", \"SEXE\", \"ILT\", \"IPONDI\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# TDM8 est catégorielle avec 'ZZZ' -> on crée une version numérique dédiée\n",
    "if \"TDM8\" in df.columns:\n",
    "    df[\"TDM8_NUM\"] = pd.to_numeric(df[\"TDM8\"].astype(\"string\").str.replace(\"ZZZ\", \"\", regex=False), errors=\"coerce\")\n",
    "\n",
    "# Strings\n",
    "for c in [\"MOCO_DET\", \"NUMF\", \"NUMMI\", \"NIVEAU_APPARIEMENT\", \"DCFLT\", \"WORK_IN_PAYS\", \"SFM\", \"PCSL\", \"NA5\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(\"string\").str.strip().str.upper()\n",
    "\n",
    "print(\"OK: types normalisés.\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) CHECK STRUCTUREL (doit être parfait après ton Code 1)\n",
    "# ============================================================\n",
    "print(\"\\n[3] Checks structurels …\")\n",
    "\n",
    "gsize = df.groupby(\"ID_COUPLE\").size()\n",
    "bad_size = (gsize != 2).sum()\n",
    "print(f\"Couples avec taille != 2: {bad_size:,} (attendu 0)\")\n",
    "\n",
    "role_tab = df.pivot_table(index=\"ID_COUPLE\", columns=\"ROLE_COUPLE\", values=\"LPRM\", aggfunc=\"size\", fill_value=0)\n",
    "for col in [\"REF\", \"CONJ\"]:\n",
    "    if col not in role_tab.columns:\n",
    "        role_tab[col] = 0\n",
    "\n",
    "bad_role = ((role_tab[\"REF\"] != 1) | (role_tab[\"CONJ\"] != 1)).sum()\n",
    "print(f\"Couples sans exactement 1 REF et 1 CONJ: {bad_role:,} (attendu 0)\")\n",
    "\n",
    "if bad_size > 0 or bad_role > 0:\n",
    "    print(\"⚠️ WARNING: structure non parfaite -> ton fichier d’entrée n’est pas celui du Code 1 strict.\")\n",
    "    # On continue quand même pour diagnostiquer.\n",
    "\n",
    "n_couples = df[\"ID_COUPLE\"].nunique()\n",
    "print(f\"Nb couples: {n_couples:,}\")\n",
    "\n",
    "# ============================================================\n",
    "# 4) CONSTRUIRE UNE TABLE COUPLE-LEVEL (REF/CONJ côte à côte)\n",
    "#    -> c’est PLUS PROPRE pour scorer et diagnostiquer\n",
    "# ============================================================\n",
    "print(\"\\n[4] Construction table couple-level (REF/CONJ côte à côte) …\")\n",
    "\n",
    "vars_for_checks = []\n",
    "for v in [\"COUPLE\", \"TYPMR\", \"TDM8_NUM\", \"STAT_CONJ\", \"MOCO_DET\", \"AGEREV\", \"ILT\", \"NUMF\", \"NIVEAU_APPARIEMENT\"]:\n",
    "    if v in df.columns:\n",
    "        vars_for_checks.append(v)\n",
    "\n",
    "wide_chk = df.pivot_table(index=\"ID_COUPLE\", columns=\"ROLE_COUPLE\", values=vars_for_checks, aggfunc=\"first\")\n",
    "wide_chk.columns = [f\"{v}_{r}\" for (v, r) in wide_chk.columns]\n",
    "wide_chk = wide_chk.reset_index()\n",
    "\n",
    "print(f\"OK: couple-level -> {wide_chk.shape[0]:,} lignes | {wide_chk.shape[1]} colonnes\")\n",
    "\n",
    "# ============================================================\n",
    "# 5) DIAGNOSTICS BRUTS (avant score)\n",
    "# ============================================================\n",
    "print(\"\\n[5] Diagnostics bruts …\")\n",
    "\n",
    "# COUPLE incohérences\n",
    "if \"COUPLE_REF\" in wide_chk.columns and \"COUPLE_CONJ\" in wide_chk.columns:\n",
    "    both1 = ((wide_chk[\"COUPLE_REF\"] == 1) & (wide_chk[\"COUPLE_CONJ\"] == 1)).mean()\n",
    "    any_not1 = ((wide_chk[\"COUPLE_REF\"] != 1) | (wide_chk[\"COUPLE_CONJ\"] != 1)).sum()\n",
    "    print(f\"COUPLE déclaratif parfait (1 & 1): {both1*100:.2f}%\")\n",
    "    print(f\"Couples avec au moins 1 membre COUPLE!=1: {any_not1:,} ({any_not1/n_couples*100:.2f}%)\")\n",
    "\n",
    "# NUMF bizarre (tu as vu 18 'Z')\n",
    "if \"NUMF_REF\" in wide_chk.columns:\n",
    "    print(\"\\nTop modalités NUMF_REF:\")\n",
    "    print(wide_chk[\"NUMF_REF\"].value_counts(dropna=False).head(5))\n",
    "\n",
    "# STAT_CONJ distribution pour que tu confirmes le mapping\n",
    "if \"STAT_CONJ_REF\" in wide_chk.columns:\n",
    "    print(\"\\nDistribution STAT_CONJ (REF) :\")\n",
    "    print(wide_chk[\"STAT_CONJ_REF\"].value_counts(dropna=False).sort_index())\n",
    "\n",
    "if \"STAT_CONJ_CONJ\" in wide_chk.columns:\n",
    "    print(\"\\nDistribution STAT_CONJ (CONJ) :\")\n",
    "    print(wide_chk[\"STAT_CONJ_CONJ\"].value_counts(dropna=False).sort_index())\n",
    "\n",
    "# ============================================================\n",
    "# 6) SCORE_QUALITE (0–5) + FLAGS détaillés\n",
    "#    Q1: les deux COUPLE==1\n",
    "#    Q2: TYPMR dans set (au moins un des deux, typiquement identique)\n",
    "#    Q3: TDM8 dans set (au moins un)\n",
    "#    Q4: au moins un STAT_CONJ dans set union\n",
    "#    Q5: les deux MOCO_DET dans set adulte\n",
    "# ============================================================\n",
    "print(\"\\n[6] Calcul SCORE_QUALITE (0–5) …\")\n",
    "\n",
    "# Q1\n",
    "if \"COUPLE_REF\" in wide_chk.columns and \"COUPLE_CONJ\" in wide_chk.columns:\n",
    "    wide_chk[\"Q1_COUPLE_11\"] = ((wide_chk[\"COUPLE_REF\"] == 1) & (wide_chk[\"COUPLE_CONJ\"] == 1)).astype(\"int8\")\n",
    "else:\n",
    "    wide_chk[\"Q1_COUPLE_11\"] = 0\n",
    "\n",
    "# Q2\n",
    "if \"TYPMR_REF\" in wide_chk.columns:\n",
    "    wide_chk[\"Q2_TYPMR\"] = wide_chk[\"TYPMR_REF\"].isin(TYPMR_COUPLE_SET).astype(\"int8\")\n",
    "elif \"TYPMR_CONJ\" in wide_chk.columns:\n",
    "    wide_chk[\"Q2_TYPMR\"] = wide_chk[\"TYPMR_CONJ\"].isin(TYPMR_COUPLE_SET).astype(\"int8\")\n",
    "else:\n",
    "    wide_chk[\"Q2_TYPMR\"] = 0\n",
    "\n",
    "# Q3\n",
    "if \"TDM8_NUM_REF\" in wide_chk.columns:\n",
    "    wide_chk[\"Q3_TDM8\"] = wide_chk[\"TDM8_NUM_REF\"].isin(TDM8_COUPLE_SET).astype(\"int8\")\n",
    "elif \"TDM8_NUM_CONJ\" in wide_chk.columns:\n",
    "    wide_chk[\"Q3_TDM8\"] = wide_chk[\"TDM8_NUM_CONJ\"].isin(TDM8_COUPLE_SET).astype(\"int8\")\n",
    "else:\n",
    "    wide_chk[\"Q3_TDM8\"] = 0\n",
    "\n",
    "# Q4 (STAT_CONJ -> union)\n",
    "if \"STAT_CONJ_REF\" in wide_chk.columns and \"STAT_CONJ_CONJ\" in wide_chk.columns:\n",
    "    wide_chk[\"Q4_STAT_CONJ\"] = (\n",
    "        wide_chk[\"STAT_CONJ_REF\"].isin(STAT_CONJ_UNION_SET) |\n",
    "        wide_chk[\"STAT_CONJ_CONJ\"].isin(STAT_CONJ_UNION_SET)\n",
    "    ).astype(\"int8\")\n",
    "else:\n",
    "    wide_chk[\"Q4_STAT_CONJ\"] = 0\n",
    "\n",
    "# Q5 MOCO_DET (les deux)\n",
    "if \"MOCO_DET_REF\" in wide_chk.columns and \"MOCO_DET_CONJ\" in wide_chk.columns:\n",
    "    wide_chk[\"Q5_MOCO\"] = (\n",
    "        wide_chk[\"MOCO_DET_REF\"].isin(MOCO_ADULTE_SET) &\n",
    "        wide_chk[\"MOCO_DET_CONJ\"].isin(MOCO_ADULTE_SET)\n",
    "    ).astype(\"int8\")\n",
    "else:\n",
    "    wide_chk[\"Q5_MOCO\"] = 0\n",
    "\n",
    "wide_chk[\"SCORE_QUALITE\"] = (\n",
    "    wide_chk[\"Q1_COUPLE_11\"] +\n",
    "    wide_chk[\"Q2_TYPMR\"] +\n",
    "    wide_chk[\"Q3_TDM8\"] +\n",
    "    wide_chk[\"Q4_STAT_CONJ\"] +\n",
    "    wide_chk[\"Q5_MOCO\"]\n",
    ").astype(\"int8\")\n",
    "\n",
    "print(\"Distribution SCORE_QUALITE :\")\n",
    "print(wide_chk[\"SCORE_QUALITE\"].value_counts().sort_index())\n",
    "print(f\"Score moyen: {wide_chk['SCORE_QUALITE'].mean():.2f} / 5\")\n",
    "\n",
    "# ============================================================\n",
    "# 7) AJOUT VARIABLES PROBIT bivarié (Y_REF / Y_CONJ) + écarts\n",
    "# ============================================================\n",
    "print(\"\\n[7] Variables analytiques (Y_REF/Y_CONJ + écart âge) …\")\n",
    "\n",
    "if \"ILT_REF\" in wide_chk.columns:\n",
    "    wide_chk[\"Y_REF\"] = (wide_chk[\"ILT_REF\"] == 7).astype(\"int8\")\n",
    "if \"ILT_CONJ\" in wide_chk.columns:\n",
    "    wide_chk[\"Y_CONJ\"] = (wide_chk[\"ILT_CONJ\"] == 7).astype(\"int8\")\n",
    "\n",
    "if \"AGEREV_REF\" in wide_chk.columns and \"AGEREV_CONJ\" in wide_chk.columns:\n",
    "    wide_chk[\"ECART_AGE_AGEREV\"] = (wide_chk[\"AGEREV_REF\"] - wide_chk[\"AGEREV_CONJ\"]).abs()\n",
    "\n",
    "if \"Y_REF\" in wide_chk.columns and \"Y_CONJ\" in wide_chk.columns:\n",
    "    ct = pd.crosstab(wide_chk[\"Y_REF\"], wide_chk[\"Y_CONJ\"], margins=True)\n",
    "    print(\"\\nTable décision transfrontalière (ILT==7):\")\n",
    "    print(ct)\n",
    "\n",
    "# ============================================================\n",
    "# 8) MERGE SCORE DANS LE LONG + EXPORTS\n",
    "# ============================================================\n",
    "print(\"\\n[8] Merge vers LONG + exports …\")\n",
    "\n",
    "df = df.merge(\n",
    "    wide_chk[[\"ID_COUPLE\", \"SCORE_QUALITE\", \"Q1_COUPLE_11\", \"Q2_TYPMR\", \"Q3_TDM8\", \"Q4_STAT_CONJ\", \"Q5_MOCO\"]]\n",
    "    , on=\"ID_COUPLE\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Export LONG enrichi\n",
    "df.to_csv(OUT_LONG_ENRICHED, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "print(f\"OK: LONG enrichi -> {OUT_LONG_ENRICHED}\")\n",
    "\n",
    "# Export WIDE validé\n",
    "wide_chk.to_csv(OUT_WIDE, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "print(f\"OK: WIDE validé -> {OUT_WIDE}\")\n",
    "\n",
    "# Sample Excel (WIDE)\n",
    "sample = wide_chk.sample(n=min(SAMPLE_N, len(wide_chk)), random_state=RANDOM_STATE).copy()\n",
    "front = [\"ID_COUPLE\", \"SCORE_QUALITE\", \"Y_REF\", \"Y_CONJ\", \"ECART_AGE_AGEREV\",\n",
    "         \"COUPLE_REF\", \"COUPLE_CONJ\", \"STAT_CONJ_REF\", \"STAT_CONJ_CONJ\",\n",
    "         \"TYPMR_REF\", \"TDM8_NUM_REF\", \"MOCO_DET_REF\", \"MOCO_DET_CONJ\",\n",
    "         \"NIVEAU_APPARIEMENT_REF\", \"NUMF_REF\"]\n",
    "front = [c for c in front if c in sample.columns]\n",
    "sample = sample[front + [c for c in sample.columns if c not in front]]\n",
    "sample.to_excel(OUT_SAMPLE_XLSX, index=False)\n",
    "print(f\"OK: Sample Excel -> {OUT_SAMPLE_XLSX}\")\n",
    "\n",
    "dt = time.time() - t0\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(f\"TERMINÉ — temps total: {dt:.1f}s\")\n",
    "print(\"=\" * 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
